# RAG_Core/api/personalized_main.py

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Optional, AsyncIterator
import logging
import json
import asyncio

from workflow.personalized_rag_workflow import PersonalizedRAGWorkflow
from database.milvus_client import milvus_client

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Personalized RAG Chatbot API",
    description="API cho h·ªá th·ªëng chatbot RAG v·ªõi c√° nh√¢n h√≥a theo th√¥ng tin kh√°ch h√†ng",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

personalized_workflow = None


# ================================================================
# SCHEMAS
# ================================================================

class ChatMessage(BaseModel):
    role: str
    content: str


class PersonalizedChatRequest(BaseModel):
    """
    Request schema cho personalized chat

    Example:
    {
        "question": "gi·∫£i ph√°p ph√π h·ª£p v·ªõi c√¥ng ty truy·ªÅn th√¥ng?",
        "history": [
            {"role": "user", "content": "Hi"},
            {"role": "assistant", "content": "Xin ch√†o..."}
        ],
        "stream": true,
        "name": "Nguyen Hoang Long",
        "introduction": "T·ªïng gi√°m ƒë·ªëc c√¥ng ty c√¥ng ngh·ªá v√† truy·ªÅn th√¥ng VTC NetViet"
    }
    """
    question: str = Field(..., description="C√¢u h·ªèi c·ªßa kh√°ch h√†ng")
    history: Optional[List[ChatMessage]] = Field(
        default=[],
        description="L·ªãch s·ª≠ h·ªôi tho·∫°i"
    )
    stream: Optional[bool] = Field(
        default=True,
        description="S·ª≠ d·ª•ng streaming mode"
    )
    name: Optional[str] = Field(
        default="",
        description="T√™n kh√°ch h√†ng"
    )
    introduction: Optional[str] = Field(
        default="",
        description="Gi·ªõi thi·ªáu v·ªÅ kh√°ch h√†ng (ch·ª©c danh, c√¥ng ty, lƒ©nh v·ª±c...)"
    )


class DocumentReference(BaseModel):
    document_id: str
    type: str
    description: Optional[str] = None
    url: Optional[str] = None
    filename: Optional[str] = None
    file_type: Optional[str] = None


class PersonalizedChatResponse(BaseModel):
    answer: str
    references: List[DocumentReference]
    status: str = "SUCCESS"
    personalized: bool = False
    customer_name: Optional[str] = None


class HealthResponse(BaseModel):
    status: str
    message: str
    database_connected: bool
    personalization_enabled: bool


# ================================================================
# STARTUP
# ================================================================

@app.on_event("startup")
async def startup_event():
    """Initialize Personalized RAG Workflow"""
    global personalized_workflow
    try:
        personalized_workflow = PersonalizedRAGWorkflow()
        logger.info("‚úÖ Personalized RAG Workflow initialized successfully")
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Failed to initialize workflow: {e}")


# ================================================================
# ENDPOINTS
# ================================================================

@app.get("/", response_model=dict)
async def root():
    """Root endpoint"""
    return {
        "service": "Personalized RAG Chatbot API",
        "version": "1.0.0",
        "port": 8502,
        "features": [
            "personalized-responses",
            "customer-profiling",
            "adaptive-tone",
            "streaming",
            "context-aware"
        ],
        "endpoints": {
            "chat": "/chat",
            "health": "/health"
        }
    }


async def generate_personalized_streaming_response(
        question: str,
        history: List,
        customer_name: str,
        customer_introduction: str
) -> AsyncIterator[str]:
    """
    Async generator cho personalized streaming response

    Args:
        question: C√¢u h·ªèi
        history: L·ªãch s·ª≠ h·ªôi tho·∫°i
        customer_name: T√™n kh√°ch h√†ng
        customer_introduction: Gi·ªõi thi·ªáu kh√°ch h√†ng

    Yields:
        SSE-formatted chunks
    """
    try:
        logger.info(f"üöÄ Starting personalized streaming")
        logger.info(f"   Customer: {customer_name}")
        logger.info(f"   Question: {question[:60]}...")

        # Send start chunk
        start_chunk = {
            "type": "start",
            "content": None,
            "references": None,
            "status": "processing",
            "personalized": bool(customer_name or customer_introduction),
            "customer_name": customer_name
        }
        yield f"data: {json.dumps(start_chunk, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.01)

        # Run personalized workflow
        result = await personalized_workflow.run_with_personalization_streaming(
            question=question,
            history=history,
            customer_name=customer_name,
            customer_introduction=customer_introduction
        )

        # Get answer stream
        answer_stream = result.get("answer_stream")
        references = result.get("references", [])
        personalized = result.get("personalized", False)

        logger.info(f"üìù Streaming answer (personalized={personalized})")

        # Stream chunks
        if answer_stream:
            chunk_count = 0
            async for chunk in answer_stream:
                if chunk:
                    chunk_count += 1
                    chunk_data = {
                        "type": "chunk",
                        "content": chunk,
                        "references": None,
                        "status": None
                    }
                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.001)

            logger.info(f"‚úÖ Streamed {chunk_count} chunks")
        else:
            logger.warning("‚ö†Ô∏è No answer stream available")
            error_chunk = {
                "type": "chunk",
                "content": "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi.",
                "references": None,
                "status": None
            }
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"

        # Send references
        if references:
            serializable_refs = []
            for ref in references:
                ref_dict = {
                    "document_id": ref.get("document_id", ""),
                    "type": ref.get("type", "DOCUMENT"),
                    "description": ref.get("description", "")
                }

                if ref.get("url"):
                    ref_dict["url"] = ref["url"]
                    ref_dict["filename"] = ref.get("filename", "")
                    ref_dict["file_type"] = ref.get("file_type", "")

                serializable_refs.append(ref_dict)

            ref_chunk = {
                "type": "references",
                "content": None,
                "references": serializable_refs,
                "status": None
            }
            yield f"data: {json.dumps(ref_chunk, ensure_ascii=False)}\n\n"
            logger.info(f"üìö Sent {len(serializable_refs)} references")

        # Send end chunk
        end_chunk = {
            "type": "end",
            "content": None,
            "references": None,
            "status": result.get("status", "SUCCESS"),
            "personalized": personalized,
            "customer_name": customer_name
        }
        yield f"data: {json.dumps(end_chunk, ensure_ascii=False)}\n\n"
        logger.info("‚úÖ Streaming completed")

    except Exception as e:
        logger.error(f"‚ùå Streaming error: {e}", exc_info=True)
        error_chunk = {
            "type": "error",
            "content": f"L·ªói: {str(e)}",
            "references": None,
            "status": "ERROR"
        }
        yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"


@app.post("/chat")
async def personalized_chat(request: PersonalizedChatRequest):
    """
    Main personalized chat endpoint

    H·ªó tr·ª£ c·∫£ streaming v√† non-streaming mode
    C√° nh√¢n h√≥a d·ª±a tr√™n name v√† introduction

    Example Request (Streaming):
    ```json
    {
        "question": "gi·∫£i ph√°p ph√π h·ª£p v·ªõi c√¥ng ty truy·ªÅn th√¥ng?",
        "history": [
            {"role": "user", "content": "Hi"},
            {"role": "assistant", "content": "Xin ch√†o..."}
        ],
        "stream": true,
        "name": "Nguyen Hoang Long",
        "introduction": "T·ªïng gi√°m ƒë·ªëc c√¥ng ty c√¥ng ngh·ªá v√† truy·ªÅn th√¥ng VTC NetViet"
    }
    ```
    """
    try:
        if not personalized_workflow:
            raise HTTPException(
                status_code=503,
                detail="Workflow not initialized"
            )

        logger.info(f"üì® Personalized chat request:")
        logger.info(f"   Question: {request.question[:100]}...")
        logger.info(f"   Customer: {request.name}")
        logger.info(f"   Stream: {request.stream}")

        # Convert history
        history = []
        for msg in request.history:
            history.append({
                "role": msg.role,
                "content": msg.content
            })

        # STREAMING MODE
        if request.stream:
            logger.info("üîÑ Using STREAMING mode with personalization")
            return StreamingResponse(
                generate_personalized_streaming_response(
                    question=request.question,
                    history=history,
                    customer_name=request.name or "",
                    customer_introduction=request.introduction or ""
                ),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no"
                }
            )

        # NON-STREAMING MODE
        logger.info("üìã Using NON-STREAMING mode with personalization")
        result = personalized_workflow.run_with_personalization(
            question=request.question,
            history=history,
            customer_name=request.name or "",
            customer_introduction=request.introduction or ""
        )

        # Prepare references
        references = []
        for ref in result.get("references", []):
            ref_obj = DocumentReference(
                document_id=ref.get("document_id", "unknown"),
                type=ref.get("type", "DOCUMENT"),
                description=ref.get("description", None),
                url=ref.get("url", None),
                filename=ref.get("filename", None),
                file_type=ref.get("file_type", None)
            )
            references.append(ref_obj)

        logger.info(f"‚úÖ Personalized response ready")
        logger.info(f"   Personalized: {result.get('personalized', False)}")

        return PersonalizedChatResponse(
            answer=result.get("answer", "L·ªói x·ª≠ l√Ω c√¢u h·ªèi"),
            references=references,
            status=result.get("status", "ERROR"),
            personalized=result.get("personalized", False),
            customer_name=request.name or None
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Chat endpoint error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}"
        )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    try:
        db_connected = False
        try:
            db_connected = milvus_client.check_connection()
        except Exception as db_error:
            logger.warning(f"Database check failed: {db_error}")

        workflow_ready = personalized_workflow is not None

        if db_connected and workflow_ready:
            return HealthResponse(
                status="healthy",
                message="H·ªá th·ªëng ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng v·ªõi personalization",
                database_connected=True,
                personalization_enabled=True
            )
        elif workflow_ready and not db_connected:
            return HealthResponse(
                status="degraded",
                message="M·∫•t k·∫øt n·ªëi c∆° s·ªü d·ªØ li·ªáu",
                database_connected=False,
                personalization_enabled=True
            )
        else:
            return HealthResponse(
                status="unhealthy",
                message="H·ªá th·ªëng g·∫∑p s·ª± c·ªë",
                database_connected=False,
                personalization_enabled=False
            )

    except Exception as e:
        logger.error(f"Health check error: {e}")
        return HealthResponse(
            status="unhealthy",
            message=f"L·ªói: {str(e)}",
            database_connected=False,
            personalization_enabled=False
        )


@app.get("/info")
async def system_info():
    """Th√¥ng tin h·ªá th·ªëng"""
    return {
        "service": "Personalized RAG API",
        "version": "1.0.0",
        "port": 8502,
        "features": {
            "personalization": {
                "enabled": True,
                "description": "C√° nh√¢n h√≥a c√¢u tr·∫£ l·ªùi theo th√¥ng tin kh√°ch h√†ng",
                "supported_fields": ["name", "introduction"]
            },
            "customer_profiling": {
                "enabled": True,
                "auto_detect": ["title", "seniority", "industry", "tone"]
            },
            "streaming": {
                "enabled": True,
                "format": "SSE (Server-Sent Events)"
            },
            "base_rag": {
                "enabled": True,
                "agents": ["FAQ", "RETRIEVER", "GRADER", "GENERATOR"]
            }
        },
        "workflow_ready": personalized_workflow is not None
    }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8502,
        log_level="info"
    )