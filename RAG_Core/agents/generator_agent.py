# RAG_Core/agents/generator_agent.py - FIXED STREAMING VERSION

from typing import Dict, Any, List, AsyncIterator
from models.llm_model import llm_model
import logging
from config.settings import settings

logger = logging.getLogger(__name__)


class GeneratorAgent:
    def __init__(self):
        self.name = "GENERATOR"

        # Standard prompt (unchanged)
        self.standard_prompt = """B·∫°n l√† m·ªôt chuy√™n vi√™n t∆∞ v·∫•n kh√°ch h√†ng ng∆∞·ªùi Vi·ªát Nam th√¢n thi·ªán v√† chuy√™n nghi·ªáp - chuy√™n gia v·ªÅ chuy·ªÉn ƒë·ªïi s·ªë v√† c√¥ng ngh·ªá.

C√¢u h·ªèi c·ªßa kh√°ch h√†ng: "{question}"

Th√¥ng tin tham kh·∫£o t·ª´ t√†i li·ªáu:
{documents}

L·ªãch s·ª≠ tr√≤ chuy·ªán g·∫ßn ƒë√¢y:
{history}

===== H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI =====

B∆Ø·ªöC 1: T·ª± ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng t√†i li·ªáu
- T√†i li·ªáu c√≥ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn c√¢u h·ªèi kh√¥ng?
- Th√¥ng tin c√≥ ƒë·ªß chi ti·∫øt ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c kh√¥ng?
- T√†i li·ªáu c√≥ gi·∫£i ƒë√°p ƒë√∫ng v·∫•n ƒë·ªÅ ng∆∞·ªùi d√πng ƒëang h·ªèi kh√¥ng?

B∆Ø·ªöC 2: Ch·ªçn c√°ch tr·∫£ l·ªùi ph√π h·ª£p

N·∫æU T√ÄI LI·ªÜU ƒê·ª¶ T·ªêT:
‚Üí Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin trong t√†i li·ªáu
‚Üí Di·ªÖn ƒë·∫°t b·∫±ng gi·ªçng vƒÉn t·ª± nhi√™n nh∆∞ ng∆∞·ªùi Vi·ªát Nam n√≥i chuy·ªán
‚Üí Tr·∫£ l·ªùi th·∫≥ng v√†o v·∫•n ƒë·ªÅ, ng·∫Øn g·ªçn s√∫c t√≠ch
‚Üí K·∫øt th√∫c b·∫±ng c√¢u h·ªèi ng·∫Øn ƒë·ªÉ ti·∫øp t·ª•c h·ªó tr·ª£ n·∫øu c·∫ßn

N·∫æU T√ÄI LI·ªÜU KH√îNG ƒê·ª¶ T·ªêT/KH√îNG LI√äN QUAN:
‚Üí B·∫Øt ƒë·∫ßu b·∫±ng: "D·ª±a tr√™n t·ªïng h·ª£p t·ª´ c√°c ngu·ªìn th√¥ng tin, c√¢u tr·∫£ l·ªùi b·∫°n c√≥ th·ªÉ tham kh·∫£o nh∆∞ sau:"
‚Üí D·ª±a tr√™n ki·∫øn th·ª©c chuy√™n m√¥n c·ªßa b·∫°n v·ªÅ chuy·ªÉn ƒë·ªïi s·ªë, cung c·∫•p:
  ‚Ä¢ C√¢u tr·∫£ l·ªùi h·ªØu √≠ch mang t√≠nh tham kh·∫£o
  ‚Ä¢ Chia s·∫ª ki·∫øn th·ª©c chung v·ªÅ ch·ªß ƒë·ªÅ (n·∫øu c√≥)
  ‚Ä¢ G·ª£i √Ω h∆∞·ªõng t√¨m hi·ªÉu ho·∫∑c gi·∫£i ph√°p thay th·∫ø
‚Üí Cu·ªëi c√πng ƒë·ªÅ xu·∫•t: "ƒê·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c h∆°n, b·∫°n vui l√≤ng li√™n h·ªá hotline: {support_phone}"

H√£y tr·∫£ l·ªùi:"""

        # Follow-up prompt (unchanged)
        self.followup_prompt = """B·∫°n l√† m·ªôt chuy√™n vi√™n t∆∞ v·∫•n kh√°ch h√†ng ng∆∞·ªùi Vi·ªát Nam th√¢n thi·ªán v√† chuy√™n nghi·ªáp - chuy√™n gia v·ªÅ chuy·ªÉn ƒë·ªïi s·ªë v√† c√¥ng ngh·ªá.

üîç NG·ªÆ C·∫¢NH CU·ªòC TR√í CHUY·ªÜN:
{context_summary}

üìù L·ªäCH S·ª¨ G·∫¶N NH·∫§T:
{recent_history}

‚ùì C√ÇU H·ªéI FOLLOW-UP: "{question}"

üìö TH√îNG TIN T√ÄI LI·ªÜU LI√äN QUAN:
{documents}

===== H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI =====

B∆Ø·ªöC 1: T·ª± ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng t√†i li·ªáu
- T√†i li·ªáu c√≥ li√™n quan ƒë·∫øn c√¢u h·ªèi follow-up n√†y kh√¥ng?
- Th√¥ng tin c√≥ ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi c·ª• th·ªÉ ph·∫ßn kh√°ch h√†ng mu·ªën bi·∫øt th√™m kh√¥ng?

B∆Ø·ªöC 2: Ch·ªçn c√°ch tr·∫£ l·ªùi ph√π h·ª£p

N·∫æU T√ÄI LI·ªÜU ƒê·ª¶ T·ªêT:
‚Üí Nh·∫≠n bi·∫øt r·∫±ng kh√°ch h√†ng ƒëang h·ªèi ti·∫øp v·ªÅ ch·ªß ƒë·ªÅ ƒë√£ th·∫£o lu·∫≠n
‚Üí Tham chi·∫øu ƒë·∫øn th√¥ng tin ƒë√£ cung c·∫•p tr∆∞·ªõc ƒë√≥ m·ªôt c√°ch t·ª± nhi√™n
‚Üí Tr·∫£ l·ªùi c·ª• th·ªÉ v√†o ph·∫ßn ƒë∆∞·ª£c h·ªèi, KH√îNG l·∫∑p l·∫°i to√†n b·ªô th√¥ng tin c≈©
‚Üí Ng·∫Øn g·ªçn, s√∫c t√≠ch, ƒë√∫ng tr·ªçng t√¢m

N·∫æU T√ÄI LI·ªÜU KH√îNG ƒê·ª¶ T·ªêT:
‚Üí B·∫Øt ƒë·∫ßu: "D·ª±a tr√™n t·ªïng h·ª£p t·ª´ c√°c ngu·ªìn th√¥ng tin, c√¢u tr·∫£ l·ªùi b·∫°n c√≥ th·ªÉ tham kh·∫£o nh∆∞ sau:"
‚Üí D·ª±a tr√™n ki·∫øn th·ª©c chuy√™n m√¥n v·ªÅ chuy·ªÉn ƒë·ªïi s·ªë ƒë·ªÉ tr·∫£ l·ªùi
‚Üí Th·ªÉ hi·ªán s·ª± chuy√™n nghi·ªáp nh∆∞ng khi√™m t·ªën
‚Üí Cu·ªëi c√πng: "ƒê·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c h∆°n, vui l√≤ng li√™n h·ªá hotline: {support_phone}"

H√£y tr·∫£ l·ªùi:"""


    def _deduplicate_references(self, references: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Lo·∫°i b·ªè references tr√πng l·∫∑p"""
        if not references:
            return []

        seen_doc_ids = set()
        unique_references = []

        for ref in references:
            doc_id = ref.get('document_id')
            if doc_id and doc_id not in seen_doc_ids:
                seen_doc_ids.add(doc_id)
                unique_references.append(ref)

        return unique_references

    def _format_documents(self, documents: List[Dict[str, Any]]) -> str:
        """Format documents th√†nh text"""
        if not documents:
            return "Kh√¥ng c√≥ t√†i li·ªáu tham kh·∫£o"

        doc_lines = []
        for i, doc in enumerate(documents[:5], 1):
            description = doc.get('description', '')
            score = doc.get('similarity_score', 0)
            doc_lines.append(f"[T√†i li·ªáu {i}] (ƒê·ªô li√™n quan: {score:.2f})\n{description}")

        return "\n\n".join(doc_lines)

    def _format_history(self, history: List, max_turns: int = 2) -> str:
        """Format l·ªãch s·ª≠ h·ªôi tho·∫°i"""
        if not history:
            return "Kh√¥ng c√≥ l·ªãch s·ª≠"

        # Normalize history
        normalized_history = []
        for msg in history:
            if isinstance(msg, dict):
                normalized_history.append({
                    "role": msg.get("role", ""),
                    "content": msg.get("content", "")
                })
            else:
                normalized_history.append({
                    "role": getattr(msg, "role", ""),
                    "content": getattr(msg, "content", "")
                })

        # L·∫•y N turn g·∫ßn nh·∫•t
        recent_history = normalized_history[-(max_turns * 2):] if len(
            normalized_history) > max_turns * 2 else normalized_history

        history_lines = []
        for msg in recent_history:
            role = "üë§ Kh√°ch h√†ng" if msg.get("role") == "user" else "ü§ñ Tr·ª£ l√Ω"
            content = msg.get("content", "")
            if content:
                history_lines.append(f"{role}: {content}")

        return "\n".join(history_lines) if history_lines else "Kh√¥ng c√≥ l·ªãch s·ª≠"

    def _extract_context_summary(self, history: List) -> str:
        """Tr√≠ch xu·∫•t context summary"""
        if not history or len(history) < 2:
            return "ƒê√¢y l√† c√¢u h·ªèi ƒë·∫ßu ti√™n"

        # Normalize history
        normalized_history = []
        for msg in history:
            if isinstance(msg, dict):
                normalized_history.append(msg)
            else:
                normalized_history.append({
                    "role": getattr(msg, "role", ""),
                    "content": getattr(msg, "content", "")
                })

        # L·∫•y c√¢u h·ªèi tr∆∞·ªõc
        for i in range(len(normalized_history) - 1, -1, -1):
            if normalized_history[i].get("role") == "user":
                prev_question = normalized_history[i].get("content", "")

                for j in range(i + 1, len(normalized_history)):
                    if normalized_history[j].get("role") == "assistant":
                        prev_answer = normalized_history[j].get("content", "")
                        return f"Ch·ªß ƒë·ªÅ ƒëang th·∫£o lu·∫≠n: {prev_question}\nƒê√£ tr·∫£ l·ªùi: {prev_answer[:200]}..."

                return f"Ch·ªß ƒë·ªÅ ƒëang th·∫£o lu·∫≠n: {prev_question}"

        return "ƒêang trong cu·ªôc tr√≤ chuy·ªán"

    def process(
            self,
            question: str,
            documents: List[Dict[str, Any]],
            references: List[Dict[str, Any]] = None,
            history: List[Dict[str, str]] = None,
            is_followup: bool = False,
            context_summary: str = "",
            **kwargs
    ) -> Dict[str, Any]:
        """Non-streaming generation (original)"""
        try:
            if not documents:
                return {
                    "status": "ERROR",
                    "answer": "Kh√¥ng c√≥ t√†i li·ªáu ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi",
                    "references": [],
                    "next_agent": "end"
                }

            doc_text = self._format_documents(documents)
            history_text = self._format_history(history or [], max_turns=2)

            # Ch·ªçn prompt
            if is_followup:
                if not context_summary:
                    context_summary = self._extract_context_summary(history or [])

                prompt = self.followup_prompt.format(
                    question=question,
                    context_summary=context_summary,
                    recent_history=history_text,
                    documents=doc_text,
                    support_phone=settings.SUPPORT_PHONE
                )
            else:
                prompt = self.standard_prompt.format(
                    question=question,
                    history=history_text,
                    documents=doc_text,
                    support_phone=settings.SUPPORT_PHONE
                )

            # Generate answer (non-streaming)
            answer = llm_model.invoke(prompt)

            if not answer or len(answer.strip()) < 10:
                answer = "T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin li√™n quan nh∆∞ng g·∫∑p kh√≥ khƒÉn trong vi·ªác t·∫°o c√¢u tr·∫£ l·ªùi."

            unique_references = self._deduplicate_references(references or [])

            return {
                "status": "SUCCESS",
                "answer": answer,
                "references": unique_references,
                "next_agent": "end"
            }

        except Exception as e:
            logger.error(f"Error in generator agent: {e}", exc_info=True)
            return {
                "status": "ERROR",
                "answer": f"L·ªói t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}",
                "references": [],
                "next_agent": "end"
            }

    async def process_streaming(
            self,
            question: str,
            documents: List[Dict[str, Any]],
            references: List[Dict[str, Any]] = None,
            history: List[Dict[str, str]] = None,
            is_followup: bool = False,
            context_summary: str = "",
            **kwargs
    ) -> AsyncIterator[str]:
        """
        FIXED: Streaming generation with proper async/await

        Returns async generator that yields text chunks
        """
        try:
            logger.info(f"üöÄ Generator: Starting streaming for: {question[:50]}...")

            if not documents:
                yield "Kh√¥ng c√≥ t√†i li·ªáu ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi."
                return

            # Format inputs
            doc_text = self._format_documents(documents)
            history_text = self._format_history(history or [], max_turns=2)

            # Choose prompt
            if is_followup:
                if not context_summary:
                    context_summary = self._extract_context_summary(history or [])

                prompt = self.followup_prompt.format(
                    question=question,
                    context_summary=context_summary,
                    recent_history=history_text,
                    documents=doc_text,
                    support_phone=settings.SUPPORT_PHONE
                )
            else:
                prompt = self.standard_prompt.format(
                    question=question,
                    history=history_text,
                    documents=doc_text,
                    support_phone=settings.SUPPORT_PHONE
                )

            logger.info(f"üìù Generator: Prompt prepared, length={len(prompt)}")

            # CRITICAL: Stream from LLM
            chunk_count = 0
            async for chunk in llm_model.astream(prompt):
                if chunk:  # Only yield non-empty chunks
                    chunk_count += 1
                    logger.debug(f"Generator yielding chunk #{chunk_count}: {chunk[:30]}...")
                    yield chunk

            logger.info(f"‚úÖ Generator: Completed streaming {chunk_count} chunks")

        except Exception as e:
            logger.error(f"‚ùå Generator streaming error: {e}", exc_info=True)
            yield f"\n\n[L·ªói: {str(e)}]"