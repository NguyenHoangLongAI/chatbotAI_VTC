# RAG_Core/agents/personalization_generator_agent.py

from typing import Dict, Any, List, AsyncIterator
from models.llm_model import llm_model
import logging

logger = logging.getLogger(__name__)


class PersonalizationGeneratorAgent:
    """
    Generator Agent v·ªõi personalization t√≠ch h·ª£p s·∫µn

    Ch·ª©c nƒÉng:
    - T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ documents
    - T·ª± ƒë·ªông c√° nh√¢n h√≥a d·ª±a tr√™n th√¥ng tin kh√°ch h√†ng
    - H·ªó tr·ª£ streaming
    """

    def __init__(self):
        self.name = "PERSONALIZATION_GENERATOR"

        # Standard prompt v·ªõi personalization
        self.personalized_standard_prompt = """B·∫°n l√† tr·ª£ l√Ω ·∫£o Onetouch - chuy√™n gia ƒë√†o t·∫°o k·ªπ nƒÉng s·ªë cho ng∆∞·ªùi d√¢n v√† doanh nghi·ªáp.

TH√îNG TIN KH√ÅCH H√ÄNG:
- T√™n: {customer_name}
- Gi·ªõi thi·ªáu: {customer_introduction}
- Ph√¢n t√≠ch: {customer_analysis}

C√ÇU H·ªéI C·ª¶A KH√ÅCH H√ÄNG: "{question}"

TH√îNG TIN T√ÄI LI·ªÜU THAM KH·∫¢O:
{documents}

L·ªäCH S·ª¨ H·ªòI THO·∫†I G·∫¶N NH·∫§T:
{history}

Y√äU C·∫¶U TR·∫¢ L·ªúI:

1. **X∆∞ng h√¥ ph√π h·ª£p**:
   - N·∫øu l√† Gi√°m ƒë·ªëc/T·ªïng gi√°m ƒë·ªëc/CEO: "Th∆∞a Anh/Ch·ªã {customer_name}"
   - N·∫øu l√† Manager/Tr∆∞·ªüng ph√≤ng: "Anh/Ch·ªã {customer_name}"
   - N·∫øu l√† nh√¢n vi√™n/c√° nh√¢n: "B·∫°n {customer_name}"
   - N·∫øu kh√¥ng r√µ: "Anh/Ch·ªã {customer_name}"

2. **C√° nh√¢n h√≥a n·ªôi dung**:
   - Li√™n k·∫øt c√¢u tr·∫£ l·ªùi v·ªõi lƒ©nh v·ª±c/ng√†nh ngh·ªÅ c·ªßa kh√°ch h√†ng
   - ƒê∆∞a ra v√≠ d·ª• ph√π h·ª£p v·ªõi context c√¥ng ty/vai tr√≤
   - ƒêi·ªÅu ch·ªânh ƒë·ªô chuy√™n s√¢u d·ª±a tr√™n v·ªã tr√≠:
     * L√£nh ƒë·∫°o ‚Üí T·ªïng quan chi·∫øn l∆∞·ª£c, t·∫ßm nh√¨n
     * Qu·∫£n l√Ω ‚Üí Gi·∫£i ph√°p c·ª• th·ªÉ, th·ª±c ti·ªÖn
     * Nh√¢n vi√™n ‚Üí Chi ti·∫øt th·ª±c h√†nh, d·ªÖ hi·ªÉu

3. **Tone ph√π h·ª£p**:
   - L√£nh ƒë·∫°o c·∫•p cao: T√¥n tr·ªçng, t∆∞ v·∫•n chi·∫øn l∆∞·ª£c
   - Qu·∫£n l√Ω: Chuy√™n nghi·ªáp, gi·∫£i ph√°p c·ª• th·ªÉ
   - Nh√¢n vi√™n: Th√¢n thi·ªán, h∆∞·ªõng d·∫´n chi ti·∫øt

4. **N·ªôi dung**:
   - Tr·∫£ l·ªùi b·∫±ng gi·ªçng vƒÉn t·ª± nhi√™n nh∆∞ ng∆∞·ªùi Vi·ªát Nam n√≥i chuy·ªán
   - Tr·∫£ l·ªùi th·∫≥ng v√†o v·∫•n ƒë·ªÅ, ng·∫Øn g·ªçn s√∫c t√≠ch
   - D·ª±a v√†o th√¥ng tin t√†i li·ªáu nh∆∞ng di·ªÖn ƒë·∫°t theo c√°ch hi·ªÉu c·ªßa b·∫°n
   - K·∫øt th√∫c b·∫±ng c√¢u h·ªèi ng·∫Øn ƒë·ªÉ ti·∫øp t·ª•c h·ªó tr·ª£ n·∫øu c·∫ßn

5. **ƒê·ªãnh d·∫°ng**:
   - B·∫ÆT ƒê·∫¶U b·∫±ng l·ªùi x∆∞ng h√¥ ph√π h·ª£p
   - N·ªôi dung ch√≠nh v·ªõi personalization
   - K·∫æT TH√öC b·∫±ng c√¢u h·ªèi m·ªü

H√£y tr·∫£ l·ªùi nh∆∞ ƒëang n√≥i chuy·ªán tr·ª±c ti·∫øp v·ªõi kh√°ch h√†ng:"""

        # Follow-up prompt v·ªõi personalization
        self.personalized_followup_prompt = """B·∫°n l√† tr·ª£ l√Ω ·∫£o Onetouch - chuy√™n gia ƒë√†o t·∫°o k·ªπ nƒÉng s·ªë cho ng∆∞·ªùi d√¢n v√† doanh nghi·ªáp.

TH√îNG TIN KH√ÅCH H√ÄNG:
- T√™n: {customer_name}
- Gi·ªõi thi·ªáu: {customer_introduction}
- Ph√¢n t√≠ch: {customer_analysis}

NG·ªÆ C·∫¢NH CU·ªòC TR√í CHUY·ªÜN:
{context_summary}

L·ªäCH S·ª¨ G·∫¶N NH·∫§T:
{recent_history}

C√ÇU H·ªéI FOLLOW-UP: "{question}"

TH√îNG TIN T√ÄI LI·ªÜU LI√äN QUAN:
{documents}

Y√äU C·∫¶U ƒê·∫∂C BI·ªÜT CHO FOLLOW-UP:

1. **X∆∞ng h√¥ nh·∫•t qu√°n** v·ªõi c√¢u tr·∫£ l·ªùi tr∆∞·ªõc (Th∆∞a Anh/Ch·ªã {customer_name})

2. **Tham chi·∫øu t·ª± nhi√™n**:
   - Nh·∫≠n bi·∫øt r·∫±ng kh√°ch h√†ng ƒëang h·ªèi ti·∫øp v·ªÅ ch·ªß ƒë·ªÅ ƒë√£ th·∫£o lu·∫≠n
   - Tham chi·∫øu ƒë·∫øn th√¥ng tin ƒë√£ cung c·∫•p tr∆∞·ªõc ƒë√≥
   - Tr·∫£ l·ªùi c·ª• th·ªÉ v√†o ph·∫ßn m√† kh√°ch h√†ng mu·ªën bi·∫øt th√™m

3. **KH√îNG l·∫∑p l·∫°i** to√†n b·ªô th√¥ng tin ƒë√£ n√≥i, ch·ªâ t·∫≠p trung v√†o ph·∫ßn ƒë∆∞·ª£c h·ªèi

4. **C√° nh√¢n h√≥a ti·∫øp t·ª•c**:
   - Duy tr√¨ tone ph√π h·ª£p v·ªõi v·ªã tr√≠ kh√°ch h√†ng
   - Li√™n k·∫øt v·ªõi ng√†nh ngh·ªÅ/lƒ©nh v·ª±c c·ªßa h·ªç

5. **N·ªôi dung**:
   - Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ƒë√∫ng tr·ªçng t√¢m
   - K·∫øt th√∫c b·∫±ng c√¢u h·ªèi ƒë·ªÉ ti·∫øp t·ª•c h·ªó tr·ª£

H√£y tr·∫£ l·ªùi:"""

    def _analyze_customer_profile(
            self,
            customer_name: str,
            customer_introduction: str
    ) -> str:
        """
        Ph√¢n t√≠ch nhanh profile kh√°ch h√†ng
        """
        try:
            intro_lower = (customer_introduction or "").lower()

            # Detect title
            if any(x in intro_lower for x in ["t·ªïng gi√°m ƒë·ªëc", "t·ªïng gd", "ceo"]):
                title = "T·ªïng gi√°m ƒë·ªëc"
                seniority = "C-level"
                tone = "formal"
            elif any(x in intro_lower for x in ["gi√°m ƒë·ªëc", "gd", "director"]):
                title = "Gi√°m ƒë·ªëc"
                seniority = "C-level"
                tone = "formal"
            elif any(x in intro_lower for x in ["tr∆∞·ªüng ph√≤ng", "tp", "manager"]):
                title = "Tr∆∞·ªüng ph√≤ng"
                seniority = "Manager"
                tone = "professional"
            elif any(x in intro_lower for x in ["nh√¢n vi√™n", "nv", "staff"]):
                title = "Nh√¢n vi√™n"
                seniority = "Staff"
                tone = "friendly"
            else:
                title = "Qu√Ω kh√°ch"
                seniority = "Individual"
                tone = "professional"

            # Detect industry
            if any(x in intro_lower for x in ["c√¥ng ngh·ªá", "technology", "tech", "cntt"]):
                industry = "C√¥ng ngh·ªá th√¥ng tin"
            elif any(x in intro_lower for x in ["truy·ªÅn th√¥ng", "media", "marketing"]):
                industry = "Truy·ªÅn th√¥ng & Marketing"
            elif any(x in intro_lower for x in ["s·∫£n xu·∫•t", "manufacturing"]):
                industry = "S·∫£n xu·∫•t"
            else:
                industry = "Kh√¥ng x√°c ƒë·ªãnh"

            return f"""
- Ch·ª©c danh: {title}
- C·∫•p ƒë·ªô: {seniority}
- Lƒ©nh v·ª±c: {industry}
- Tone khuy·∫øn ngh·ªã: {tone}
"""
        except Exception as e:
            logger.error(f"Error analyzing profile: {e}")
            return "- Ch·ª©c danh: Qu√Ω kh√°ch\n- C·∫•p ƒë·ªô: Individual\n- Tone: professional"

    def _format_documents(self, documents: List[Dict[str, Any]]) -> str:
        """Format documents"""
        if not documents:
            return "Kh√¥ng c√≥ t√†i li·ªáu tham kh·∫£o"

        doc_lines = []
        for i, doc in enumerate(documents[:5], 1):
            description = doc.get('description', '')
            score = doc.get('similarity_score', 0)
            doc_lines.append(f"[T√†i li·ªáu {i}] (ƒê·ªô li√™n quan: {score:.2f})\n{description}")

        return "\n\n".join(doc_lines)

    def _format_history(self, history: List, max_turns: int = 2) -> str:
        """Format history"""
        if not history:
            return "Kh√¥ng c√≥ l·ªãch s·ª≠"

        normalized_history = []
        for msg in history:
            if isinstance(msg, dict):
                normalized_history.append({
                    "role": msg.get("role", ""),
                    "content": msg.get("content", "")
                })
            else:
                normalized_history.append({
                    "role": getattr(msg, "role", ""),
                    "content": getattr(msg, "content", "")
                })

        recent = normalized_history[-(max_turns * 2):] if len(
            normalized_history) > max_turns * 2 else normalized_history

        history_lines = []
        for msg in recent:
            role = "üë§ Kh√°ch h√†ng" if msg.get("role") == "user" else "ü§ñ Tr·ª£ l√Ω"
            content = msg.get("content", "")
            if content:
                history_lines.append(f"{role}: {content}")

        return "\n".join(history_lines) if history_lines else "Kh√¥ng c√≥ l·ªãch s·ª≠"

    def _extract_context_summary(self, history: List) -> str:
        """Extract context summary"""
        if not history or len(history) < 2:
            return "ƒê√¢y l√† c√¢u h·ªèi ƒë·∫ßu ti√™n"

        normalized_history = []
        for msg in history:
            if isinstance(msg, dict):
                normalized_history.append(msg)
            else:
                normalized_history.append({
                    "role": getattr(msg, "role", ""),
                    "content": getattr(msg, "content", "")
                })

        for i in range(len(normalized_history) - 1, -1, -1):
            if normalized_history[i].get("role") == "user":
                prev_question = normalized_history[i].get("content", "")

                for j in range(i + 1, len(normalized_history)):
                    if normalized_history[j].get("role") == "assistant":
                        prev_answer = normalized_history[j].get("content", "")
                        return f"Ch·ªß ƒë·ªÅ ƒëang th·∫£o lu·∫≠n: {prev_question}\nƒê√£ tr·∫£ l·ªùi: {prev_answer[:200]}..."

                return f"Ch·ªß ƒë·ªÅ ƒëang th·∫£o lu·∫≠n: {prev_question}"

        return "ƒêang trong cu·ªôc tr√≤ chuy·ªán"

    def process(
            self,
            question: str,
            documents: List[Dict[str, Any]],
            customer_name: str = "",
            customer_introduction: str = "",
            references: List[Dict[str, Any]] = None,
            history: List[Dict[str, str]] = None,
            is_followup: bool = False,
            context_summary: str = "",
            **kwargs
    ) -> Dict[str, Any]:
        """
        Non-streaming generation v·ªõi personalization
        """
        try:
            logger.info("üé≠ Personalization Generator (non-streaming)")
            logger.info(f"   Customer: {customer_name}")
            logger.info(f"   Follow-up: {is_followup}")

            if not documents:
                return {
                    "status": "ERROR",
                    "answer": "Kh√¥ng c√≥ t√†i li·ªáu ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi",
                    "references": [],
                    "next_agent": "end"
                }

            # Analyze customer
            customer_analysis = self._analyze_customer_profile(
                customer_name,
                customer_introduction
            )

            # Format inputs
            doc_text = self._format_documents(documents)
            history_text = self._format_history(history or [], max_turns=2)

            # Choose prompt
            if is_followup:
                if not context_summary:
                    context_summary = self._extract_context_summary(history or [])

                prompt = self.personalized_followup_prompt.format(
                    customer_name=customer_name or "Qu√Ω kh√°ch",
                    customer_introduction=customer_introduction or "Kh√¥ng c√≥ th√¥ng tin",
                    customer_analysis=customer_analysis,
                    question=question,
                    context_summary=context_summary,
                    recent_history=history_text,
                    documents=doc_text
                )
            else:
                prompt = self.personalized_standard_prompt.format(
                    customer_name=customer_name or "Qu√Ω kh√°ch",
                    customer_introduction=customer_introduction or "Kh√¥ng c√≥ th√¥ng tin",
                    customer_analysis=customer_analysis,
                    question=question,
                    history=history_text,
                    documents=doc_text
                )

            # Generate answer
            answer = llm_model.invoke(prompt)

            if not answer or len(answer.strip()) < 10:
                answer = "T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin li√™n quan nh∆∞ng g·∫∑p kh√≥ khƒÉn trong vi·ªác t·∫°o c√¢u tr·∫£ l·ªùi."

            logger.info("‚úÖ Personalized answer generated")

            return {
                "status": "SUCCESS",
                "answer": answer,
                "references": references or [],
                "personalized": True,
                "customer_name": customer_name,
                "next_agent": "end"
            }

        except Exception as e:
            logger.error(f"‚ùå Generator error: {e}", exc_info=True)
            return {
                "status": "ERROR",
                "answer": f"L·ªói t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}",
                "references": [],
                "next_agent": "end"
            }

    async def process_streaming(
            self,
            question: str,
            documents: List[Dict[str, Any]],
            customer_name: str = "",
            customer_introduction: str = "",
            references: List[Dict[str, Any]] = None,
            history: List[Dict[str, str]] = None,
            is_followup: bool = False,
            context_summary: str = "",
            **kwargs
    ) -> AsyncIterator[str]:
        """
        Streaming generation v·ªõi personalization
        """
        try:
            logger.info("üé≠ Personalization Generator streaming")
            logger.info(f"   Customer: {customer_name}")
            logger.info(f"   Follow-up: {is_followup}")

            if not documents:
                yield "Kh√¥ng c√≥ t√†i li·ªáu ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi."
                return

            # Analyze customer
            customer_analysis = self._analyze_customer_profile(
                customer_name,
                customer_introduction
            )

            # Format inputs
            doc_text = self._format_documents(documents)
            history_text = self._format_history(history or [], max_turns=2)

            # Choose prompt
            if is_followup:
                if not context_summary:
                    context_summary = self._extract_context_summary(history or [])

                prompt = self.personalized_followup_prompt.format(
                    customer_name=customer_name or "Qu√Ω kh√°ch",
                    customer_introduction=customer_introduction or "Kh√¥ng c√≥ th√¥ng tin",
                    customer_analysis=customer_analysis,
                    question=question,
                    context_summary=context_summary,
                    recent_history=history_text,
                    documents=doc_text
                )
            else:
                prompt = self.personalized_standard_prompt.format(
                    customer_name=customer_name or "Qu√Ω kh√°ch",
                    customer_introduction=customer_introduction or "Kh√¥ng c√≥ th√¥ng tin",
                    customer_analysis=customer_analysis,
                    question=question,
                    history=history_text,
                    documents=doc_text
                )

            logger.info("üöÄ Streaming personalized answer...")

            # Stream from LLM
            chunk_count = 0
            async for chunk in llm_model.astream(prompt):
                if chunk:
                    chunk_count += 1
                    logger.debug(f"Generator chunk #{chunk_count}: {chunk[:30]}...")
                    yield chunk

            logger.info(f"‚úÖ Generator streaming completed: {chunk_count} chunks")

        except Exception as e:
            logger.error(f"‚ùå Generator streaming error: {e}", exc_info=True)
            yield f"\n\n[L·ªói: {str(e)}]"