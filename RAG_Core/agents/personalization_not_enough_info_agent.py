# RAG_Core/agents/personalization_not_enough_info_agent.py

from typing import Dict, Any, List, AsyncIterator
from models.llm_model import llm_model
from config.settings import settings
import logging

logger = logging.getLogger(__name__)


class PersonalizationNotEnoughInfoAgent:
    """
    NotEnoughInfoAgent v·ªõi personalization - C√° nh√¢n h√≥a c√¢u tr·∫£ l·ªùi khi kh√¥ng ƒë·ªß th√¥ng tin

    Ch·ª©c nƒÉng:
    - Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung khi kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu
    - C√° nh√¢n h√≥a theo th√¥ng tin kh√°ch h√†ng (t√™n, ch·ª©c danh, lƒ©nh v·ª±c)
    - ƒêi·ªÅu ch·ªânh tone ph√π h·ª£p v·ªõi v·ªã tr√≠ kh√°ch h√†ng
    - H·ªó tr·ª£ streaming
    """

    def __init__(self):
        self.name = "PERSONALIZATION_NOT_ENOUGH_INFO"

        # Personalized prompt
        self.personalized_prompt = """B·∫°n l√† tr·ª£ l√Ω ·∫£o Onetouch - chuy√™n gia ƒë√†o t·∫°o k·ªπ nƒÉng s·ªë cho ng∆∞·ªùi d√¢n v√† doanh nghi·ªáp.

TH√îNG TIN KH√ÅCH H√ÄNG:
- T√™n: {customer_name}
- Gi·ªõi thi·ªáu: {customer_introduction}
- Ph√¢n t√≠ch: {customer_analysis}

T√åNH HU·ªêNG: Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu trong h·ªá th·ªëng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c c√¢u h·ªèi n√†y.

C√ÇU H·ªéI C·ª¶A KH√ÅCH H√ÄNG: "{question}"

Y√äU C·∫¶U TR·∫¢ L·ªúI:

1. **X∆∞ng h√¥ ph√π h·ª£p**:
   - N·∫øu l√† Gi√°m ƒë·ªëc/T·ªïng gi√°m ƒë·ªëc/CEO: "Th∆∞a Anh/Ch·ªã {customer_name}"
   - N·∫øu l√† Manager/Tr∆∞·ªüng ph√≤ng: "Th∆∞a Anh/Ch·ªã {customer_name}"
   - N·∫øu l√† nh√¢n vi√™n/c√° nh√¢n: "B·∫°n {customer_name}"
   - N·∫øu kh√¥ng r√µ: "Th∆∞a Anh/Ch·ªã {customer_name}"

2. **C·∫•u tr√∫c c√¢u tr·∫£ l·ªùi** (B·∫ÆT BU·ªòC NG·∫ÆN G·ªåN - t·ªëi ƒëa 3-4 c√¢u):
   a) M·ªû ƒê·∫¶U (1 c√¢u):
      "Th∆∞a Anh/Ch·ªã {customer_name}, d·ª±a tr√™n t·ªïng h·ª£p t·ª´ c√°c ngu·ªìn th√¥ng tin, b·∫°n c√≥ th·ªÉ tham kh·∫£o nh∆∞ sau:"

   b) N·ªòI DUNG CH√çNH (1-2 c√¢u):
      - Cung c·∫•p th√¥ng tin mang t√≠nh tham kh·∫£o chung
      - KH√îNG suy ƒëo√°n chi ti·∫øt k·ªπ thu·∫≠t
      - KH√îNG ph√¢n t√≠ch d√†i d√≤ng
      - Li√™n k·∫øt v·ªõi lƒ©nh v·ª±c/ng√†nh ngh·ªÅ c·ªßa kh√°ch h√†ng (n·∫øu c√≥)

   c) K·∫æT TH√öC (1 c√¢u):
      "ƒê·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c h∆°n, Anh/Ch·ªã vui l√≤ng li√™n h·ªá hotline {support_phone}."

3. **C√° nh√¢n h√≥a n·ªôi dung**:
   - N·∫øu bi·∫øt lƒ©nh v·ª±c: ƒê∆∞a v√≠ d·ª• ph√π h·ª£p (c√¥ng ngh·ªá, truy·ªÅn th√¥ng, s·∫£n xu·∫•t...)
   - ƒêi·ªÅu ch·ªânh ƒë·ªô k·ªπ thu·∫≠t theo v·ªã tr√≠:
     * L√£nh ƒë·∫°o ‚Üí T·ªïng quan, chi·∫øn l∆∞·ª£c
     * Qu·∫£n l√Ω ‚Üí Gi·∫£i ph√°p th·ª±c t·∫ø
     * Nh√¢n vi√™n ‚Üí D·ªÖ hi·ªÉu, ·ª©ng d·ª•ng

4. **Tone ph√π h·ª£p**:
   - L√£nh ƒë·∫°o c·∫•p cao: T√¥n tr·ªçng, chuy√™n nghi·ªáp
   - Qu·∫£n l√Ω: Th√¢n thi·ªán, h·ªó tr·ª£
   - Nh√¢n vi√™n: G·∫ßn g≈©i, d·ªÖ ti·∫øp c·∫≠n

5. **Y√äU C·∫¶U ƒê·∫∂C BI·ªÜT**:
   - NG·∫ÆN G·ªåN (t·ªëi ƒëa 3-4 c√¢u)
   - KH√îNG k·ªÉ v√≠ d·ª• d√†i
   - KH√îNG gi·∫£i th√≠ch chi ti·∫øt
   - B·∫ÆT ƒê·∫¶U b·∫±ng l·ªùi x∆∞ng h√¥ ph√π h·ª£p
   - K·∫æT TH√öC b·∫±ng ƒë·ªÅ ngh·ªã li√™n h·ªá hotline

H√£y tr·∫£ l·ªùi:"""

    def _analyze_customer_profile(
            self,
            customer_name: str,
            customer_introduction: str
    ) -> str:
        """
        Ph√¢n t√≠ch nhanh profile kh√°ch h√†ng
        """
        try:
            intro_lower = (customer_introduction or "").lower()

            # Detect title
            if any(x in intro_lower for x in ["t·ªïng gi√°m ƒë·ªëc", "t·ªïng gd", "ceo"]):
                title = "T·ªïng gi√°m ƒë·ªëc"
                seniority = "C-level"
                tone = "formal"
            elif any(x in intro_lower for x in ["gi√°m ƒë·ªëc", "gd", "director"]):
                title = "Gi√°m ƒë·ªëc"
                seniority = "C-level"
                tone = "formal"
            elif any(x in intro_lower for x in ["tr∆∞·ªüng ph√≤ng", "tp", "manager"]):
                title = "Tr∆∞·ªüng ph√≤ng"
                seniority = "Manager"
                tone = "professional"
            elif any(x in intro_lower for x in ["nh√¢n vi√™n", "nv", "staff"]):
                title = "Nh√¢n vi√™n"
                seniority = "Staff"
                tone = "friendly"
            else:
                title = "Qu√Ω kh√°ch"
                seniority = "Individual"
                tone = "professional"

            # Detect industry
            if any(x in intro_lower for x in ["c√¥ng ngh·ªá", "technology", "tech", "cntt"]):
                industry = "C√¥ng ngh·ªá th√¥ng tin"
            elif any(x in intro_lower for x in ["truy·ªÅn th√¥ng", "media", "marketing"]):
                industry = "Truy·ªÅn th√¥ng & Marketing"
            elif any(x in intro_lower for x in ["s·∫£n xu·∫•t", "manufacturing"]):
                industry = "S·∫£n xu·∫•t"
            else:
                industry = "Kh√¥ng x√°c ƒë·ªãnh"

            return f"""
- Ch·ª©c danh: {title}
- C·∫•p ƒë·ªô: {seniority}
- Lƒ©nh v·ª±c: {industry}
- Tone khuy·∫øn ngh·ªã: {tone}
"""
        except Exception as e:
            logger.error(f"Error analyzing profile: {e}")
            return "- Ch·ª©c danh: Qu√Ω kh√°ch\n- C·∫•p ƒë·ªô: Individual\n- Tone: professional"

    def process(
            self,
            question: str,
            customer_name: str = "",
            customer_introduction: str = "",
            **kwargs
    ) -> Dict[str, Any]:
        """
        Non-streaming process v·ªõi personalization

        Args:
            question: C√¢u h·ªèi
            customer_name: T√™n kh√°ch h√†ng
            customer_introduction: Gi·ªõi thi·ªáu v·ªÅ kh√°ch h√†ng
        """
        try:
            logger.info("üé≠ Personalized Not Enough Info (non-streaming)")
            logger.info(f"   Customer: {customer_name}")

            # Analyze customer profile
            customer_analysis = self._analyze_customer_profile(
                customer_name,
                customer_introduction
            )

            # Create personalized prompt
            prompt = self.personalized_prompt.format(
                customer_name=customer_name or "Qu√Ω kh√°ch",
                customer_introduction=customer_introduction or "Kh√¥ng c√≥ th√¥ng tin",
                customer_analysis=customer_analysis,
                question=question,
                support_phone=settings.SUPPORT_PHONE
            )

            # Generate answer
            logger.info("ü§ñ Generating personalized answer (not enough info)...")

            answer = llm_model.invoke(
                prompt,
                temperature=0.2,  # Th·∫•p ƒë·ªÉ tu√¢n th·ªß format
                top_p=0.7,
                max_tokens=150,  # Gi·ªõi h·∫°n ƒë·ªô d√†i
                frequency_penalty=0.5,
                presence_penalty=0.0
            )

            logger.info("‚úÖ Personalized answer generated")

            return {
                "status": "SUCCESS",
                "answer": answer,
                "references": [
                    {
                        "document_id": "llm_knowledge",
                        "type": "GENERAL_KNOWLEDGE"
                    }
                ],
                "personalized": True,
                "customer_name": customer_name,
                "next_agent": "end"
            }

        except Exception as e:
            logger.error(f"‚ùå Personalized Not Enough Info error: {e}")

            # Fallback answer
            fallback_greeting = f"Th∆∞a Anh/Ch·ªã {customer_name}" if customer_name else "Xin ch√†o"

            return {
                "status": "ERROR",
                "answer": f"""{fallback_greeting},

Xin l·ªói, h·ªá th·ªëng g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n.

ƒê·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ t·ªët nh·∫•t, vui l√≤ng li√™n h·ªá hotline: {settings.SUPPORT_PHONE}

C·∫£m ∆°n b·∫°n!""",
                "references": [],
                "personalized": bool(customer_name),
                "next_agent": "end"
            }

    async def process_streaming(
            self,
            question: str,
            customer_name: str = "",
            customer_introduction: str = "",
            **kwargs
    ) -> AsyncIterator[str]:
        """
        Streaming process v·ªõi personalization

        Args:
            question: C√¢u h·ªèi
            customer_name: T√™n kh√°ch h√†ng
            customer_introduction: Gi·ªõi thi·ªáu v·ªÅ kh√°ch h√†ng
        """
        try:
            logger.info("üé≠ Personalized Not Enough Info streaming")
            logger.info(f"   Customer: {customer_name}")

            # Analyze customer profile
            customer_analysis = self._analyze_customer_profile(
                customer_name,
                customer_introduction
            )

            # Create personalized prompt
            prompt = self.personalized_prompt.format(
                customer_name=customer_name or "Qu√Ω kh√°ch",
                customer_introduction=customer_introduction or "Kh√¥ng c√≥ th√¥ng tin",
                customer_analysis=customer_analysis,
                question=question,
                support_phone=settings.SUPPORT_PHONE
            )

            logger.info("üöÄ Streaming personalized answer...")

            # Stream from LLM
            chunk_count = 0
            async for chunk in llm_model.astream(prompt):
                if chunk:
                    chunk_count += 1
                    logger.debug(f"Not Enough Info chunk #{chunk_count}: {chunk[:30]}...")
                    yield chunk

            logger.info(f"‚úÖ Not Enough Info streaming completed: {chunk_count} chunks")

        except Exception as e:
            logger.error(f"‚ùå Streaming error: {e}", exc_info=True)

            # Fallback streaming
            fallback_greeting = f"Th∆∞a Anh/Ch·ªã {customer_name}" if customer_name else "Xin ch√†o"
            error_message = f"""{fallback_greeting},

Xin l·ªói, h·ªá th·ªëng g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n.

ƒê·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ t·ªët nh·∫•t, vui l√≤ng li√™n h·ªá hotline: {settings.SUPPORT_PHONE}

C·∫£m ∆°n b·∫°n!"""

            yield error_message