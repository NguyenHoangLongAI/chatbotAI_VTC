from typing import Dict, Any, List
import logging
import json
import re

from models.llm_model import llm_model
from tools.vector_search import check_database_connection

logger = logging.getLogger(__name__)


class SupervisorAgent:
    """
    SUPERVISOR AGENT
    - Resolve follow-up question
    - Replace ambiguous references (th√†nh ph·∫ßn th·ª© X, n√≥, ph·∫ßn n√†y...)
    - Classify agent
    """

    def __init__(self):
        self.name = "SUPERVISOR"

        self.prompt_template = """
B·∫°n l√† CHUY√äN GIA ph√¢n t√≠ch h·ªôi tho·∫°i v√† ƒëi·ªÅu ph·ªëi chatbot ƒë√†o t·∫°o chuy·ªÉn ƒë·ªïi s·ªë.

=====================
NHI·ªÜM V·ª§ B·∫ÆT BU·ªòC
=====================
1. Ph√¢n t√≠ch c√¢u h·ªèi hi·ªán t·∫°i d·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i.
2. X√°c ƒë·ªãnh c√¢u h·ªèi c√≥ ph·∫£i FOLLOW-UP hay kh√¥ng.
3. N·∫øu l√† follow-up:
   - Truy v·∫øt l·ªãch s·ª≠ ƒë·ªÉ x√°c ƒë·ªãnh ch√≠nh x√°c ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c nh·∫Øc t·ªõi.
   - ƒê·∫∑c bi·ªát ch√∫ √Ω c√°c c·ª•m:
     "th√†nh ph·∫ßn th·ª© X", "ph·∫ßn n√†y", "n√≥", "√Ω tr√™n", "c√°i ƒë√≥"
   - N·∫øu l·ªãch s·ª≠ c√≥ DANH S√ÅCH ƒê√ÅNH S·ªê ‚Üí √°nh x·∫° theo ƒê√öNG TH·ª® T·ª∞.
   - Vi·∫øt l·∫°i c√¢u h·ªèi R√ï NGHƒ®A, KH√îNG ƒê·∫†I T·ª™.
4. N·∫øu kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ l√†m r√µ ‚Üí gi·ªØ nguy√™n c√¢u h·ªèi g·ªëc.

=====================
PH√ÇN LO·∫†I AGENT
=====================
- FAQ: ƒë√†o t·∫°o k·ªπ nƒÉng s·ªë, chuy·ªÉn ƒë·ªïi s·ªë cho ng∆∞·ªùi d√¢n / doanh nghi·ªáp, ki·∫øn th·ª©c c√¥ng ngh·ªá th√¥ng tin
- CHATTER: c·∫£m x√∫c ti√™u c·ª±c, c·∫ßn l√†m d·ªãu
- REPORTER: l·ªói h·ªá th·ªëng, s·ª± c·ªë k·ªπ thu·∫≠t
- OTHER: ngo√†i ph·∫°m vi

=====================
INPUT
=====================
C√¢u h·ªèi hi·ªán t·∫°i:
"{question}"

L·ªãch s·ª≠ h·ªôi tho·∫°i (m·ªõi ‚Üí c≈©):
{history}

Tr·∫°ng th√°i h·ªá th·ªëng:
{system_status}

=====================
OUTPUT (JSON ONLY)
=====================
{{
  "is_followup": true | false,
  "contextualized_question": "...",
  "context_summary": "...",
  "agent": "FAQ | CHATTER | REPORTER | OTHER"
}}

=====================
V√ç D·ª§ B·∫ÆT BU·ªòC
=====================
L·ªãch s·ª≠:
1. C∆° s·ªü h·∫° t·∫ßng
2. H·ªá th·ªëng qu·∫£n l√Ω
3. C√¥ng c·ª• khai th√°c d·ªØ li·ªáu

C√¢u h·ªèi:
"Chi ti·∫øt v·ªÅ th√†nh ph·∫ßn th·ª© 2"

‚û° contextualized_question:
"Chi ti·∫øt v·ªÅ h·ªá th·ªëng qu·∫£n l√Ω trong n·ªÅn t·∫£ng chuy·ªÉn ƒë·ªïi s·ªë cho doanh nghi·ªáp"

CH·ªà TR·∫¢ V·ªÄ JSON. KH√îNG GI·∫¢I TH√çCH.
"""

    # =========================
    # PUBLIC API
    # =========================
    def classify_request(
        self,
        question: str,
        history: List[Dict[str, str]] = None
    ) -> Dict[str, Any]:

        try:
            logger.info("üë®‚Äçüíº SUPERVISOR START")
            logger.info(f"Question: {question}")

            # 1. Check system status
            db_status = check_database_connection.invoke({})
            if not db_status.get("connected", False):
                return self._reporter_response(question)

            # 2. Format history
            history_text = self._format_history(history or [])

            # 3. Build prompt
            prompt = self.prompt_template.format(
                question=question,
                history=history_text,
                system_status="B√¨nh th∆∞·ªùng"
            )

            # 4. Call LLM
            logger.info("ü§ñ Calling LLM (context resolution + classification)")
            raw_response = llm_model.invoke(prompt)

            # 5. Parse JSON
            parsed = self._parse_json(raw_response)

            # 6. Validate output
            return self._normalize_output(parsed, question)

        except Exception as e:
            logger.error("‚ùå Supervisor error", exc_info=True)
            return self._fallback_response(question)

    # =========================
    # INTERNAL METHODS
    # =========================
    def _format_history(self, history: List[Any]) -> str:
        """
        Format history an to√†n cho c·∫£:
        - dict: {"role": "...", "content": "..."}
        - ChatMessage / HumanMessage / AIMessage (LangChain, Pydantic)
        """

        if not history:
            return "Kh√¥ng c√≥ l·ªãch s·ª≠"

        recent = history[-6:]
        lines = []

        for msg in recent:
            # CASE 1: dict
            if isinstance(msg, dict):
                role = "Ng∆∞·ªùi d√πng" if msg.get("role") == "user" else "Tr·ª£ l√Ω"
                content = msg.get("content", "")

            # CASE 2: ChatMessage / LangChain message
            else:
                role_attr = getattr(msg, "role", None)
                content = getattr(msg, "content", "")

                role = "Ng∆∞·ªùi d√πng" if role_attr == "user" else "Tr·ª£ l√Ω"

            if content:
                lines.append(f"{role}: {content[:300]}")

        return "\n".join(lines) if lines else "Kh√¥ng c√≥ l·ªãch s·ª≠"

    def _parse_json(self, text: str) -> Dict[str, Any]:
        """
        Extract JSON object from LLM output
        """
        try:
            match = re.search(r"\{.*\}", text, re.DOTALL)
            if not match:
                raise ValueError("No JSON found")

            return json.loads(match.group(0))

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è JSON parse failed: {e}")
            return {}

    def _normalize_output(
        self,
        parsed: Dict[str, Any],
        original_question: str
    ) -> Dict[str, Any]:

        agent = parsed.get("agent", "FAQ").upper()
        if agent not in {"FAQ", "CHATTER", "REPORTER", "OTHER"}:
            agent = "FAQ"

        contextualized_question = parsed.get(
            "contextualized_question",
            original_question
        ).strip() or original_question

        return {
            "agent": agent,
            "is_followup": bool(parsed.get("is_followup", False)),
            "contextualized_question": contextualized_question,
            "context_summary": parsed.get("context_summary", "")
        }

    def _reporter_response(self, question: str) -> Dict[str, Any]:
        return {
            "agent": "REPORTER",
            "is_followup": False,
            "contextualized_question": question,
            "context_summary": "H·ªá th·ªëng m·∫•t k·∫øt n·ªëi"
        }

    def _fallback_response(self, question: str) -> Dict[str, Any]:
        return {
            "agent": "FAQ",
            "is_followup": False,
            "contextualized_question": question,
            "context_summary": "Fallback do l·ªói x·ª≠ l√Ω supervisor"
        }