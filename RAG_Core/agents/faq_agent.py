# RAG_Core/agents/faq_agent.py - ALWAYS USE LLM WITH STREAMING

from typing import Dict, Any, List, AsyncIterator
from models.llm_model import llm_model
from tools.vector_search import search_faq, rerank_faq
from config.settings import settings
import logging

logger = logging.getLogger(__name__)


class FAQAgent:
    def __init__(self):
        self.name = "FAQ"

        # Ng∆∞·ª°ng cho c√°c giai ƒëo·∫°n
        self.vector_threshold = 0.5
        self.rerank_threshold = 0.6

        # REMOVED: direct_answer_threshold, force_similarity_threshold
        # ‚Üí Lu√¥n d√πng LLM

        self.llm_prompt = """B·∫°n l√† m·ªôt chuy√™n vi√™n t∆∞ v·∫•n kh√°ch h√†ng ng∆∞·ªùi Vi·ªát Nam th√¢n thi·ªán v√† chuy√™n nghi·ªáp.

C√¢u h·ªèi ng∆∞·ªùi d√πng: "{question}"

K·∫øt qu·∫£ t√¨m ki·∫øm FAQ (ƒë√£ ƒë∆∞·ª£c rerank):
{faq_results}

H∆∞·ªõng d·∫´n:
1. K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp theo ƒë·ªô ph√π h·ª£p (rerank_score)
2. H√£y d·ª±a v√†o FAQ c√≥ rerank_score cao nh·∫•t ƒë·ªÉ tr·∫£ l·ªùi
3. N·∫øu kh√¥ng c√≥ FAQ n√†o ph√π h·ª£p (t·∫•t c·∫£ score qu√° th·∫•p), tr·∫£ v·ªÅ "NOT_FOUND"
4. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán v√† ch√≠nh x√°c
5. C√≥ th·ªÉ k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu FAQ n·∫øu c·∫ßn
6. ƒê·ª´ng n√≥i "D·ª±a v√†o FAQ..." hay "Theo t√†i li·ªáu..." - tr·∫£ l·ªùi tr·ª±c ti·∫øp nh∆∞ b·∫°n bi·∫øt

Tr·∫£ l·ªùi:"""

    def process(
            self,
            question: str,
            is_followup: bool = False,
            context: str = "",
            **kwargs
    ) -> Dict[str, Any]:
        """
        Non-streaming process (for backward compatibility)
        """
        try:
            logger.info("=" * 50)
            logger.info("ü§ñ FAQ AGENT PROCESSING (NON-STREAMING)")
            logger.info("=" * 50)
            logger.info(f"üìù Question: '{question[:100]}'")

            # Vector search
            faq_results = search_faq.invoke({"query": question})

            if not faq_results or "error" in str(faq_results):
                logger.warning("‚ùå Vector search failed")
                return self._route_to_retriever("Vector search failed")

            # Filter by threshold
            filtered_faqs = [
                faq for faq in faq_results
                if faq.get("similarity_score", 0) >= self.vector_threshold
            ]

            if not filtered_faqs:
                logger.info(f"‚ö†Ô∏è  No FAQ passed vector threshold {self.vector_threshold}")
                return self._route_to_retriever("No FAQ above vector threshold")

            logger.info(f"‚úÖ Found {len(filtered_faqs)} FAQs above threshold")

            # Rerank
            logger.info(f"üéØ Reranking with Cohere")
            reranked_faqs = rerank_faq.invoke({
                "query": question,
                "faq_results": filtered_faqs
            })

            if not reranked_faqs:
                logger.error("‚ùå Reranking returned empty results")
                raise RuntimeError("FAQ reranking failed")

            best_faq = reranked_faqs[0]
            rerank_score = best_faq.get("rerank_score", 0)
            similarity_score = best_faq.get("similarity_score", 0)

            logger.info(f"üìä Best FAQ Scores:")
            logger.info(f"   Rerank:     {rerank_score:.3f}")
            logger.info(f"   Similarity: {similarity_score:.3f}")

            # Check if confident enough
            if rerank_score < self.rerank_threshold:
                logger.info(f"‚ö†Ô∏è  Not confident: rerank={rerank_score:.3f} < {self.rerank_threshold}")
                return self._route_to_retriever(f"Rerank score too low: {rerank_score:.3f}")

            # ALWAYS USE LLM (no direct answer)
            logger.info(f"ü§ñ Using LLM to generate answer")

            faq_text = self._format_reranked_faq(reranked_faqs[:3])

            prompt = self.llm_prompt.format(
                question=question,
                faq_results=faq_text
            )

            response = llm_model.invoke(prompt)

            if "NOT_FOUND" in response.upper():
                logger.info("üîÑ LLM determined FAQ not sufficient ‚Üí RETRIEVER")
                return self._route_to_retriever("LLM rejected FAQ")

            if not response or len(response.strip()) < 10:
                logger.warning("‚ö†Ô∏è  Generated answer too short ‚Üí RETRIEVER")
                return self._route_to_retriever("Answer too short")

            logger.info(f"‚úÖ FAQ answer generated via LLM")
            logger.info("=" * 50 + "\n")

            return {
                "status": "SUCCESS",
                "answer": response,
                "mode": "llm",
                "references": [
                    {
                        "document_id": best_faq.get("faq_id"),
                        "type": "FAQ",
                        "description": best_faq.get("question", ""),
                        "rerank_score": round(rerank_score, 4),
                        "similarity_score": round(similarity_score, 4)
                    }
                ],
                "next_agent": "end"
            }

        except RuntimeError as e:
            logger.error(f"‚ùå Critical FAQ error: {e}")
            raise

        except Exception as e:
            logger.error(f"‚ùå Unexpected error in FAQ agent: {e}", exc_info=True)
            raise RuntimeError(f"FAQ agent failed: {e}") from e

    async def process_streaming(
            self,
            question: str,
            reranked_faqs: List[Dict[str, Any]] = None,
            is_followup: bool = False,
            context: str = "",
            **kwargs
    ) -> AsyncIterator[str]:
        """
        REAL STREAMING: Always use LLM streaming
        """
        try:
            logger.info("ü§ñ FAQ AGENT STREAMING (LLM ONLY)")

            # ‚úÖ SKIP search + rerank
            if not reranked_faqs:
                yield "Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi."
                return

            logger.info(f"üìä Received {len(reranked_faqs)} pre-ranked FAQs")

            # ‚úÖ CH·ªà STREAM T·ª™ LLM
            faq_text = self._format_reranked_faq(reranked_faqs[:3])
            prompt = self.llm_prompt.format(question=question, faq_results=faq_text)

            async for chunk in llm_model.astream(prompt):
                if chunk:
                    yield chunk

        except Exception as e:
            logger.error(f"‚ùå FAQ streaming error: {e}", exc_info=True)
            yield f"\n\n[L·ªói FAQ: {str(e)}]"

    def _format_reranked_faq(self, faq_results: List[Dict[str, Any]]) -> str:
        """Format FAQ ƒë√£ ƒë∆∞·ª£c rerank"""
        if not faq_results:
            return "Kh√¥ng t√¨m th·∫•y FAQ ph√π h·ª£p"

        formatted_lines = []
        for i, faq in enumerate(faq_results, 1):
            question = faq.get('question', '')
            answer = faq.get('answer', '')
            rerank_score = faq.get('rerank_score', 0)
            similarity_score = faq.get('similarity_score', 0)

            formatted_lines.append(
                f"FAQ {i} (Rerank: {rerank_score:.3f}, Similarity: {similarity_score:.3f}):\n"
                f"Q: {question}\n"
                f"A: {answer}\n"
            )

        return "\n".join(formatted_lines)

    def _route_to_retriever(self, reason: str) -> Dict[str, Any]:
        """Route to retriever"""
        logger.info(f"‚Üí Routing to RETRIEVER: {reason}")
        return {
            "status": "NOT_FOUND",
            "answer": "",
            "references": [],
            "next_agent": "RETRIEVER"
        }

    def set_thresholds(
            self,
            vector_threshold: float = None,
            rerank_threshold: float = None
    ):
        """Update thresholds"""
        if vector_threshold is not None:
            self.vector_threshold = vector_threshold
            logger.info(f"Vector threshold updated to {vector_threshold}")

        if rerank_threshold is not None:
            self.rerank_threshold = rerank_threshold
            logger.info(f"Rerank threshold updated to {rerank_threshold}")