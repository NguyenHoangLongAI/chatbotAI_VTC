# RAG_Core/agents/personalization_agent.py

from typing import Dict, Any, AsyncIterator
from models.llm_model import llm_model
import logging

logger = logging.getLogger(__name__)


class PersonalizationAgent:
    """
    Agent c√° nh√¢n h√≥a c√¢u tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin kh√°ch h√†ng

    Nhi·ªám v·ª•:
    - Ph√¢n t√≠ch name v√† introduction ƒë·ªÉ x√°c ƒë·ªãnh context kh√°ch h√†ng
    - ƒêi·ªÅu ch·ªânh tone, t·ª´ x∆∞ng h√¥, v√† n·ªôi dung ph√π h·ª£p
    - T√≠ch h·ª£p th√¥ng tin c√° nh√¢n v√†o c√¢u tr·∫£ l·ªùi m·ªôt c√°ch t·ª± nhi√™n
    """

    def __init__(self):
        self.name = "PERSONALIZATION"

        # Prompt template cho personalization
        self.personalization_prompt = """B·∫°n l√† tr·ª£ l√Ω ·∫£o Onetouch - chuy√™n gia ƒë√†o t·∫°o k·ªπ nƒÉng s·ªë cho ng∆∞·ªùi d√¢n v√† doanh nghi·ªáp.

TH√îNG TIN KH√ÅCH H√ÄNG:
- T√™n: {customer_name}
- Gi·ªõi thi·ªáu: {customer_introduction}

PH√ÇN T√çCH KH√ÅCH H√ÄNG:
{customer_analysis}

C√ÇU H·ªéI C·ª¶A KH√ÅCH H√ÄNG:
"{question}"

L·ªäCH S·ª¨ H·ªòI THO·∫†I:
{history}

C√ÇU TR·∫¢ L·ªúI G·ªêC (t·ª´ RAG system):
{original_answer}

NHI·ªÜM V·ª§ C·ª¶A B·∫†N:
1. **X∆∞ng h√¥ ph√π h·ª£p**:
   - N·∫øu l√† Gi√°m ƒë·ªëc/T·ªïng gi√°m ƒë·ªëc/CEO: "Th∆∞a Anh/Ch·ªã [T√™n]"
   - N·∫øu l√† Manager/Tr∆∞·ªüng ph√≤ng: "Anh/Ch·ªã [T√™n]"
   - N·∫øu l√† nh√¢n vi√™n/c√° nh√¢n: "B·∫°n [T√™n]"
   - N·∫øu kh√¥ng r√µ: "Anh/Ch·ªã [T√™n]"

2. **C√° nh√¢n h√≥a n·ªôi dung**:
   - Li√™n k·∫øt c√¢u tr·∫£ l·ªùi v·ªõi lƒ©nh v·ª±c/ng√†nh ngh·ªÅ c·ªßa kh√°ch h√†ng
   - ƒê∆∞a ra v√≠ d·ª• ph√π h·ª£p v·ªõi context c√¥ng ty/vai tr√≤
   - ƒêi·ªÅu ch·ªânh ƒë·ªô chuy√™n s√¢u d·ª±a tr√™n v·ªã tr√≠ (l√£nh ƒë·∫°o ‚Üí t·ªïng quan chi·∫øn l∆∞·ª£c, nh√¢n vi√™n ‚Üí chi ti·∫øt th·ª±c h√†nh)

3. **Tone ph√π h·ª£p**:
   - L√£nh ƒë·∫°o c·∫•p cao: T√¥n tr·ªçng, t∆∞ v·∫•n chi·∫øn l∆∞·ª£c, t·∫ßm nh√¨n
   - Qu·∫£n l√Ω: Chuy√™n nghi·ªáp, th·ª±c ti·ªÖn, gi·∫£i ph√°p c·ª• th·ªÉ
   - Nh√¢n vi√™n: Th√¢n thi·ªán, h∆∞·ªõng d·∫´n chi ti·∫øt, d·ªÖ hi·ªÉu

4. **Gi·ªØ nguy√™n th√¥ng tin ch√≠nh x√°c** t·ª´ c√¢u tr·∫£ l·ªùi g·ªëc - CH·ªà th√™m ph·∫ßn c√° nh√¢n h√≥a

Y√äU C·∫¶U ƒê·ªäNH D·∫†NG:
- B·∫ÆT ƒê·∫¶U b·∫±ng l·ªùi x∆∞ng h√¥ ph√π h·ª£p
- N·ªôi dung ch√≠nh: T√≠ch h·ª£p c√¢u tr·∫£ l·ªùi g·ªëc v·ªõi context c√° nh√¢n
- K·∫æT TH√öC: C√¢u h·ªèi m·ªü ƒë·ªÉ ti·∫øp t·ª•c h·ªó tr·ª£

H√ÉY TR·∫¢ L·ªúI:"""

        # Prompt ph√¢n t√≠ch kh√°ch h√†ng
        self.analysis_prompt = """Ph√¢n t√≠ch th√¥ng tin kh√°ch h√†ng sau:

T√äN: {customer_name}
GI·ªöI THI·ªÜU: {customer_introduction}

H√£y tr·∫£ v·ªÅ JSON format:
{{
    "title": "T·ªïng gi√°m ƒë·ªëc/Gi√°m ƒë·ªëc/Tr∆∞·ªüng ph√≤ng/Nh√¢n vi√™n/C√° nh√¢n",
    "company_type": "C√¥ng ngh·ªá/Truy·ªÅn th√¥ng/S·∫£n xu·∫•t/D·ªãch v·ª•/...",
    "seniority_level": "C-level/Manager/Staff/Individual",
    "industry_focus": "M√¥ t·∫£ ng·∫Øn g·ªçn ng√†nh/lƒ©nh v·ª±c",
    "addressing": "Anh/Ch·ªã",
    "tone_recommendation": "formal/professional/friendly"
}}

CH·ªà TR·∫¢ V·ªÄ JSON, KH√îNG GI·∫¢I TH√çCH TH√äM."""

    def analyze_customer_profile(
            self,
            customer_name: str,
            customer_introduction: str
    ) -> Dict[str, str]:
        """
        Ph√¢n t√≠ch profile kh√°ch h√†ng ƒë·ªÉ x√°c ƒë·ªãnh c√°ch x∆∞ng h√¥ v√† tone

        Args:
            customer_name: T√™n kh√°ch h√†ng
            customer_introduction: Gi·ªõi thi·ªáu v·ªÅ kh√°ch h√†ng

        Returns:
            Dict ch·ª©a th√¥ng tin ph√¢n t√≠ch
        """
        try:
            # N·∫øu kh√¥ng c√≥ th√¥ng tin ‚Üí default
            if not customer_name and not customer_introduction:
                return {
                    "title": "Qu√Ω kh√°ch",
                    "company_type": "Kh√¥ng x√°c ƒë·ªãnh",
                    "seniority_level": "Individual",
                    "industry_focus": "Chung",
                    "addressing": "Anh/Ch·ªã",
                    "tone_recommendation": "professional"
                }

            # G·ªçi LLM ƒë·ªÉ ph√¢n t√≠ch
            prompt = self.analysis_prompt.format(
                customer_name=customer_name or "Kh√¥ng cung c·∫•p",
                customer_introduction=customer_introduction or "Kh√¥ng cung c·∫•p"
            )

            analysis_result = llm_model.invoke(
                prompt,
                temperature=0.1,
                max_tokens=200
            )

            # Parse JSON
            import json
            import re

            # T√¨m JSON block
            json_match = re.search(r'\{[^}]+\}', analysis_result, re.DOTALL)
            if json_match:
                analysis_data = json.loads(json_match.group(0))
                logger.info(f"‚úÖ Customer analysis: {analysis_data}")
                return analysis_data

            # Fallback n·∫øu parse fail
            logger.warning("Failed to parse customer analysis, using defaults")
            return self._extract_basic_info(customer_name, customer_introduction)

        except Exception as e:
            logger.error(f"Error analyzing customer profile: {e}")
            return self._extract_basic_info(customer_name, customer_introduction)

    def _extract_basic_info(
            self,
            customer_name: str,
            customer_introduction: str
    ) -> Dict[str, str]:
        """Fallback: ph√¢n t√≠ch ƒë∆°n gi·∫£n b·∫±ng pattern matching"""
        intro_lower = (customer_introduction or "").lower()

        # Detect title
        title = "Qu√Ω kh√°ch"
        addressing = "Anh/Ch·ªã"
        seniority = "Individual"
        tone = "professional"

        if any(x in intro_lower for x in ["t·ªïng gi√°m ƒë·ªëc", "t·ªïng gd", "ceo"]):
            title = "T·ªïng gi√°m ƒë·ªëc"
            seniority = "C-level"
            tone = "formal"
        elif any(x in intro_lower for x in ["gi√°m ƒë·ªëc", "gd", "director"]):
            title = "Gi√°m ƒë·ªëc"
            seniority = "C-level"
            tone = "formal"
        elif any(x in intro_lower for x in ["tr∆∞·ªüng ph√≤ng", "tp", "manager"]):
            title = "Tr∆∞·ªüng ph√≤ng"
            seniority = "Manager"
            tone = "professional"
        elif any(x in intro_lower for x in ["nh√¢n vi√™n", "nv", "staff"]):
            title = "Nh√¢n vi√™n"
            seniority = "Staff"
            tone = "friendly"

        # Detect industry
        industry = "Kh√¥ng x√°c ƒë·ªãnh"
        if any(x in intro_lower for x in ["c√¥ng ngh·ªá", "technology", "tech", "cntt"]):
            industry = "C√¥ng ngh·ªá th√¥ng tin"
        elif any(x in intro_lower for x in ["truy·ªÅn th√¥ng", "media", "marketing"]):
            industry = "Truy·ªÅn th√¥ng & Marketing"
        elif any(x in intro_lower for x in ["s·∫£n xu·∫•t", "manufacturing"]):
            industry = "S·∫£n xu·∫•t"

        return {
            "title": title,
            "company_type": industry,
            "seniority_level": seniority,
            "industry_focus": industry,
            "addressing": addressing,
            "tone_recommendation": tone
        }

    def _format_history(self, history: list, max_turns: int = 2) -> str:
        """Format l·ªãch s·ª≠ h·ªôi tho·∫°i"""
        if not history:
            return "Kh√¥ng c√≥ l·ªãch s·ª≠"

        recent = history[-(max_turns * 2):] if len(history) > max_turns * 2 else history

        lines = []
        for msg in recent:
            role = msg.get("role", "")
            content = msg.get("content", "")

            if role == "user":
                lines.append(f"Kh√°ch h√†ng: {content}")
            elif role == "assistant":
                lines.append(f"Tr·ª£ l√Ω: {content}")

        return "\n".join(lines) if lines else "Kh√¥ng c√≥ l·ªãch s·ª≠"

    async def personalize_streaming(
            self,
            original_answer: str,
            question: str,
            customer_name: str = "",
            customer_introduction: str = "",
            history: list = None
    ) -> AsyncIterator[str]:
        """
        Streaming personalization - c√° nh√¢n h√≥a c√¢u tr·∫£ l·ªùi

        Args:
            original_answer: C√¢u tr·∫£ l·ªùi g·ªëc t·ª´ RAG system
            question: C√¢u h·ªèi c·ªßa kh√°ch h√†ng
            customer_name: T√™n kh√°ch h√†ng
            customer_introduction: Gi·ªõi thi·ªáu kh√°ch h√†ng
            history: L·ªãch s·ª≠ h·ªôi tho·∫°i

        Yields:
            Chunks c·ªßa c√¢u tr·∫£ l·ªùi ƒë√£ c√° nh√¢n h√≥a
        """
        try:
            logger.info("üé≠ Starting personalization streaming...")
            logger.info(f"   Customer: {customer_name}")
            logger.info(f"   Introduction: {customer_introduction[:50]}...")

            # Ph√¢n t√≠ch profile kh√°ch h√†ng
            customer_analysis = self.analyze_customer_profile(
                customer_name,
                customer_introduction
            )

            # Format analysis th√†nh text
            analysis_text = f"""
- Ch·ª©c danh: {customer_analysis.get('title')}
- C·∫•p ƒë·ªô: {customer_analysis.get('seniority_level')}
- Lƒ©nh v·ª±c: {customer_analysis.get('industry_focus')}
- X∆∞ng h√¥: {customer_analysis.get('addressing')}
- Tone: {customer_analysis.get('tone_recommendation')}
"""

            # Format history
            history_text = self._format_history(history or [])

            # T·∫°o prompt
            prompt = self.personalization_prompt.format(
                customer_name=customer_name or "Qu√Ω kh√°ch",
                customer_introduction=customer_introduction or "Kh√¥ng c√≥ th√¥ng tin",
                customer_analysis=analysis_text,
                question=question,
                history=history_text,
                original_answer=original_answer
            )

            logger.info("üöÄ Streaming personalized answer...")

            # Stream t·ª´ LLM
            chunk_count = 0
            async for chunk in llm_model.astream(prompt):
                if chunk:
                    chunk_count += 1
                    logger.debug(f"Personalization chunk #{chunk_count}: {chunk[:30]}...")
                    yield chunk

            logger.info(f"‚úÖ Personalization completed: {chunk_count} chunks")

        except Exception as e:
            logger.error(f"‚ùå Personalization streaming error: {e}", exc_info=True)
            # Fallback: tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi g·ªëc
            yield f"\n\nTh∆∞a {customer_name or 'Anh/Ch·ªã'},\n\n"
            yield original_answer

    def personalize(
            self,
            original_answer: str,
            question: str,
            customer_name: str = "",
            customer_introduction: str = "",
            history: list = None
    ) -> str:
        """
        Non-streaming personalization

        Args:
            original_answer: C√¢u tr·∫£ l·ªùi g·ªëc
            question: C√¢u h·ªèi
            customer_name: T√™n kh√°ch h√†ng
            customer_introduction: Gi·ªõi thi·ªáu
            history: L·ªãch s·ª≠

        Returns:
            C√¢u tr·∫£ l·ªùi ƒë√£ c√° nh√¢n h√≥a
        """
        try:
            # Ph√¢n t√≠ch profile
            customer_analysis = self.analyze_customer_profile(
                customer_name,
                customer_introduction
            )

            analysis_text = f"""
- Ch·ª©c danh: {customer_analysis.get('title')}
- C·∫•p ƒë·ªô: {customer_analysis.get('seniority_level')}
- Lƒ©nh v·ª±c: {customer_analysis.get('industry_focus')}
- X∆∞ng h√¥: {customer_analysis.get('addressing')}
- Tone: {customer_analysis.get('tone_recommendation')}
"""

            history_text = self._format_history(history or [])

            prompt = self.personalization_prompt.format(
                customer_name=customer_name or "Qu√Ω kh√°ch",
                customer_introduction=customer_introduction or "Kh√¥ng c√≥ th√¥ng tin",
                customer_analysis=analysis_text,
                question=question,
                history=history_text,
                original_answer=original_answer
            )

            # G·ªçi LLM
            personalized_answer = llm_model.invoke(
                prompt,
                temperature=0.3,
                max_tokens=1500
            )

            logger.info("‚úÖ Personalization completed (non-streaming)")
            return personalized_answer

        except Exception as e:
            logger.error(f"‚ùå Personalization error: {e}")
            # Fallback
            return f"Th∆∞a {customer_name or 'Anh/Ch·ªã'},\n\n{original_answer}"