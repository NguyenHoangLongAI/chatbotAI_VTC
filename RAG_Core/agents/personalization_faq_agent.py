# RAG_Core/agents/personalization_faq_agent.py

from typing import Dict, Any, List, AsyncIterator
from models.llm_model import llm_model
from tools.vector_search import search_faq, rerank_faq
from config.settings import settings
import logging

logger = logging.getLogger(__name__)


class PersonalizationFAQAgent:
    """
    FAQ Agent vá»›i personalization tÃ­ch há»£p sáºµn

    Chá»©c nÄƒng:
    - TÃ¬m kiáº¿m FAQ phÃ¹ há»£p
    - Tá»± Ä‘á»™ng cÃ¡ nhÃ¢n hÃ³a cÃ¢u tráº£ lá»i dá»±a trÃªn thÃ´ng tin khÃ¡ch hÃ ng
    - Há»— trá»£ streaming
    """

    def __init__(self):
        self.name = "PERSONALIZATION_FAQ"

        # Thresholds
        self.vector_threshold = 0.5
        self.rerank_threshold = 0.6

        # Prompt vá»›i personalization tÃ­ch há»£p
        self.personalized_prompt = """Báº¡n lÃ  trá»£ lÃ½ áº£o Onetouch - chuyÃªn gia Ä‘Ã o táº¡o ká»¹ nÄƒng sá»‘ cho ngÆ°á»i dÃ¢n vÃ  doanh nghiá»‡p.

THÃ”NG TIN KHÃCH HÃ€NG:
- TÃªn: {customer_name}
- Giá»›i thiá»‡u: {customer_introduction}
- PhÃ¢n tÃ­ch: {customer_analysis}

CÃ‚U Há»I Cá»¦A KHÃCH HÃ€NG: "{question}"

Káº¾T QUáº¢ TÃŒM KIáº¾M FAQ (Ä‘Ã£ Ä‘Æ°á»£c rerank):
{faq_results}

NHIá»†M Vá»¤ Cá»¦A Báº N:
1. **XÆ°ng hÃ´ phÃ¹ há»£p**:
   - Náº¿u lÃ  GiÃ¡m Ä‘á»‘c/Tá»•ng giÃ¡m Ä‘á»‘c/CEO: "ThÆ°a Anh/Chá»‹ {customer_name}"
   - Náº¿u lÃ  Manager/TrÆ°á»Ÿng phÃ²ng: "Anh/Chá»‹ {customer_name}"
   - Náº¿u lÃ  nhÃ¢n viÃªn/cÃ¡ nhÃ¢n: "Báº¡n {customer_name}"
   - Náº¿u khÃ´ng rÃµ: "Anh/Chá»‹ {customer_name}"

2. **Tráº£ lá»i dá»±a vÃ o FAQ cÃ³ rerank_score cao nháº¥t**
   - Náº¿u khÃ´ng cÃ³ FAQ phÃ¹ há»£p (táº¥t cáº£ score quÃ¡ tháº¥p), tráº£ vá» "NOT_FOUND"
   - Náº¿u cÃ³ FAQ phÃ¹ há»£p, dÃ¹ng ná»™i dung Ä‘Ã³ Ä‘á»ƒ tráº£ lá»i

3. **CÃ¡ nhÃ¢n hÃ³a ná»™i dung**:
   - LiÃªn káº¿t vá»›i lÄ©nh vá»±c/ngÃ nh nghá» cá»§a khÃ¡ch hÃ ng
   - ÄÆ°a ra vÃ­ dá»¥ phÃ¹ há»£p vá»›i context cÃ´ng ty/vai trÃ²
   - Äiá»u chá»‰nh tone phÃ¹ há»£p vá»›i vá»‹ trÃ­ (lÃ£nh Ä‘áº¡o â†’ chiáº¿n lÆ°á»£c, nhÃ¢n viÃªn â†’ thá»±c hÃ nh)

4. **Tone phÃ¹ há»£p**:
   - LÃ£nh Ä‘áº¡o cáº¥p cao: TÃ´n trá»ng, tÆ° váº¥n chiáº¿n lÆ°á»£c
   - Quáº£n lÃ½: ChuyÃªn nghiá»‡p, giáº£i phÃ¡p cá»¥ thá»ƒ
   - NhÃ¢n viÃªn: ThÃ¢n thiá»‡n, hÆ°á»›ng dáº«n chi tiáº¿t

5. **Káº¿t thÃºc**: CÃ¢u há»i má»Ÿ Ä‘á»ƒ tiáº¿p tá»¥c há»— trá»£

YÃŠU Cáº¦U QUAN TRá»ŒNG:
- Báº®T Äáº¦U báº±ng lá»i xÆ°ng hÃ´ phÃ¹ há»£p
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn, thÃ¢n thiá»‡n
- KHÃ”NG nÃ³i "Dá»±a vÃ o FAQ..." - tráº£ lá»i nhÆ° báº¡n biáº¿t
- Giá»¯ nguyÃªn thÃ´ng tin chÃ­nh xÃ¡c tá»« FAQ

HÃ£y tráº£ lá»i:"""

    def _analyze_customer_profile(
            self,
            customer_name: str,
            customer_introduction: str
    ) -> str:
        """
        PhÃ¢n tÃ­ch nhanh profile khÃ¡ch hÃ ng
        """
        try:
            intro_lower = (customer_introduction or "").lower()

            # Detect title
            if any(x in intro_lower for x in ["tá»•ng giÃ¡m Ä‘á»‘c", "tá»•ng gd", "ceo"]):
                title = "Tá»•ng giÃ¡m Ä‘á»‘c"
                seniority = "C-level"
                tone = "formal"
            elif any(x in intro_lower for x in ["giÃ¡m Ä‘á»‘c", "gd", "director"]):
                title = "GiÃ¡m Ä‘á»‘c"
                seniority = "C-level"
                tone = "formal"
            elif any(x in intro_lower for x in ["trÆ°á»Ÿng phÃ²ng", "tp", "manager"]):
                title = "TrÆ°á»Ÿng phÃ²ng"
                seniority = "Manager"
                tone = "professional"
            elif any(x in intro_lower for x in ["nhÃ¢n viÃªn", "nv", "staff"]):
                title = "NhÃ¢n viÃªn"
                seniority = "Staff"
                tone = "friendly"
            else:
                title = "QuÃ½ khÃ¡ch"
                seniority = "Individual"
                tone = "professional"

            # Detect industry
            if any(x in intro_lower for x in ["cÃ´ng nghá»‡", "technology", "tech", "cntt"]):
                industry = "CÃ´ng nghá»‡ thÃ´ng tin"
            elif any(x in intro_lower for x in ["truyá»n thÃ´ng", "media", "marketing"]):
                industry = "Truyá»n thÃ´ng & Marketing"
            elif any(x in intro_lower for x in ["sáº£n xuáº¥t", "manufacturing"]):
                industry = "Sáº£n xuáº¥t"
            else:
                industry = "KhÃ´ng xÃ¡c Ä‘á»‹nh"

            return f"""
- Chá»©c danh: {title}
- Cáº¥p Ä‘á»™: {seniority}
- LÄ©nh vá»±c: {industry}
- Tone khuyáº¿n nghá»‹: {tone}
"""
        except Exception as e:
            logger.error(f"Error analyzing profile: {e}")
            return "- Chá»©c danh: QuÃ½ khÃ¡ch\n- Cáº¥p Ä‘á»™: Individual\n- Tone: professional"

    def process(
            self,
            question: str,
            customer_name: str = "",
            customer_introduction: str = "",
            is_followup: bool = False,
            context: str = "",
            **kwargs
    ) -> Dict[str, Any]:
        """
        Non-streaming process vá»›i personalization
        """
        try:
            logger.info("=" * 50)
            logger.info("ğŸ­ PERSONALIZATION FAQ AGENT")
            logger.info("=" * 50)
            logger.info(f"ğŸ“ Question: '{question[:100]}'")
            logger.info(f"ğŸ‘¤ Customer: {customer_name}")

            # Vector search
            faq_results = search_faq.invoke({"query": question})

            if not faq_results or "error" in str(faq_results):
                logger.warning("âŒ Vector search failed")
                return self._route_to_retriever("Vector search failed")

            # Filter by threshold
            filtered_faqs = [
                faq for faq in faq_results
                if faq.get("similarity_score", 0) >= self.vector_threshold
            ]

            if not filtered_faqs:
                logger.info(f"âš ï¸  No FAQ passed vector threshold {self.vector_threshold}")
                return self._route_to_retriever("No FAQ above threshold")

            logger.info(f"âœ… Found {len(filtered_faqs)} FAQs above threshold")

            # Rerank
            logger.info(f"ğŸ¯ Reranking with Cohere")
            reranked_faqs = rerank_faq.invoke({
                "query": question,
                "faq_results": filtered_faqs
            })

            if not reranked_faqs:
                logger.error("âŒ Reranking returned empty results")
                return self._route_to_retriever("Rerank failed")

            best_faq = reranked_faqs[0]
            rerank_score = best_faq.get("rerank_score", 0)

            logger.info(f"ğŸ“Š Best FAQ rerank score: {rerank_score:.3f}")

            # Check confidence
            if rerank_score < self.rerank_threshold:
                logger.info(f"âš ï¸  Not confident: {rerank_score:.3f} < {self.rerank_threshold}")
                return self._route_to_retriever(f"Rerank score too low: {rerank_score:.3f}")

            # Analyze customer profile
            customer_analysis = self._analyze_customer_profile(
                customer_name,
                customer_introduction
            )

            # Format FAQ results
            faq_text = self._format_reranked_faq(reranked_faqs[:3])

            # Create personalized prompt
            prompt = self.personalized_prompt.format(
                customer_name=customer_name or "QuÃ½ khÃ¡ch",
                customer_introduction=customer_introduction or "KhÃ´ng cÃ³ thÃ´ng tin",
                customer_analysis=customer_analysis,
                question=question,
                faq_results=faq_text
            )

            # Generate personalized answer
            logger.info(f"ğŸ¤– Generating personalized FAQ answer")
            response = llm_model.invoke(prompt)

            if "NOT_FOUND" in response.upper():
                logger.info("ğŸ”„ LLM determined FAQ not sufficient â†’ RETRIEVER")
                return self._route_to_retriever("LLM rejected FAQ")

            if not response or len(response.strip()) < 10:
                logger.warning("âš ï¸  Generated answer too short â†’ RETRIEVER")
                return self._route_to_retriever("Answer too short")

            logger.info(f"âœ… Personalized FAQ answer generated")
            logger.info("=" * 50 + "\n")

            return {
                "status": "SUCCESS",
                "answer": response,
                "mode": "personalized_faq",
                "references": [
                    {
                        "document_id": best_faq.get("faq_id"),
                        "type": "FAQ",
                        "description": best_faq.get("question", ""),
                        "rerank_score": round(rerank_score, 4)
                    }
                ],
                "personalized": True,
                "customer_name": customer_name,
                "next_agent": "end"
            }

        except Exception as e:
            logger.error(f"âŒ Personalization FAQ error: {e}", exc_info=True)
            raise RuntimeError(f"Personalization FAQ failed: {e}") from e

    async def process_streaming(
            self,
            question: str,
            reranked_faqs: List[Dict[str, Any]],
            customer_name: str = "",
            customer_introduction: str = "",
            is_followup: bool = False,
            context: str = "",
            **kwargs
    ) -> AsyncIterator[str]:
        """
        Streaming vá»›i personalization
        """
        try:
            logger.info("ğŸ­ Personalization FAQ streaming")
            logger.info(f"   Customer: {customer_name}")

            if not reranked_faqs:
                yield "KhÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i phÃ¹ há»£p."
                return

            # Analyze customer
            customer_analysis = self._analyze_customer_profile(
                customer_name,
                customer_introduction
            )

            # Format FAQs
            faq_text = self._format_reranked_faq(reranked_faqs[:3])

            # Create prompt
            prompt = self.personalized_prompt.format(
                customer_name=customer_name or "QuÃ½ khÃ¡ch",
                customer_introduction=customer_introduction or "KhÃ´ng cÃ³ thÃ´ng tin",
                customer_analysis=customer_analysis,
                question=question,
                faq_results=faq_text
            )

            logger.info("ğŸš€ Streaming personalized FAQ answer...")

            # Stream from LLM
            chunk_count = 0
            async for chunk in llm_model.astream(prompt):
                if chunk:
                    chunk_count += 1
                    logger.debug(f"FAQ chunk #{chunk_count}: {chunk[:30]}...")
                    yield chunk

            logger.info(f"âœ… FAQ streaming completed: {chunk_count} chunks")

        except Exception as e:
            logger.error(f"âŒ FAQ streaming error: {e}", exc_info=True)
            yield f"\n\n[Lá»—i FAQ: {str(e)}]"

    def _format_reranked_faq(self, faq_results: List[Dict[str, Any]]) -> str:
        """Format FAQ results"""
        if not faq_results:
            return "KhÃ´ng tÃ¬m tháº¥y FAQ phÃ¹ há»£p"

        formatted_lines = []
        for i, faq in enumerate(faq_results, 1):
            question = faq.get('question', '')
            answer = faq.get('answer', '')
            rerank_score = faq.get('rerank_score', 0)

            formatted_lines.append(
                f"FAQ {i} (Rerank: {rerank_score:.3f}):\n"
                f"Q: {question}\n"
                f"A: {answer}\n"
            )

        return "\n".join(formatted_lines)

    def _route_to_retriever(self, reason: str) -> Dict[str, Any]:
        """Route to retriever"""
        logger.info(f"â†’ Routing to RETRIEVER: {reason}")
        return {
            "status": "NOT_FOUND",
            "answer": "",
            "references": [],
            "next_agent": "RETRIEVER"
        }