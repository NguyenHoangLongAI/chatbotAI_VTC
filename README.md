# RAG Multi-Agent Chatbot System

H·ªá th·ªëng chatbot RAG (Retrieval-Augmented Generation) s·ª≠ d·ª•ng ki·∫øn tr√∫c multi-agent v·ªõi LangGraph, ch·∫°y ho√†n to√†n local tr√™n GPU v·ªõi model GPT-OSS-20B v√† Vietnamese-SBERT embeddings.

## Ki·∫øn tr√∫c h·ªá th·ªëng

### Multi-Agent Architecture
- **SUPERVISOR**: ƒêi·ªÅu ph·ªëi ch√≠nh, ph√¢n lo·∫°i y√™u c·∫ßu v√† x·ª≠ l√Ω context
- **FAQ**: T√¨m ki·∫øm c√¢u h·ªèi th∆∞·ªùng g·∫∑p v·ªõi cross-encoder reranking
- **RETRIEVER**: T√¨m ki·∫øm t√†i li·ªáu t·ª´ vector database
- **GRADER**: ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng th√¥ng tin v·ªõi BGE Reranker v2-m3
- **GENERATOR**: T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ th√¥ng tin ƒë√£ l·ªçc (h·ªó tr·ª£ streaming)
- **NOT_ENOUGH_INFO**: X·ª≠ l√Ω thi·∫øu th√¥ng tin
- **CHATTER**: An ·ªßi c·∫£m x√∫c ti√™u c·ª±c kh√°ch h√†ng
- **REPORTER**: Th√¥ng b√°o l·ªói/b·∫£o tr√¨ h·ªá th·ªëng
- **OTHER**: X·ª≠ l√Ω y√™u c·∫ßu ngo√†i ph·∫°m vi

### Tech Stack

#### Backend Services
- **Vector Database**: Milvus v2.4.4
- **Object Storage**: MinIO
- **Coordination**: etcd v3.5.23
- **Vector DB UI**: Attu

#### AI/ML Models
- **Embedding Model**: keepitreal/vietnamese-sbert (768D)
- **LLM**: GPT-OSS-20B via Ollama 0.12.0
- **Reranker**: BAAI/bge-reranker-v2-m3 (Vietnamese-optimized)
- **OCR**: EasyOCR + Tesseract

#### Document Processing
- **Docling**: v2.17.0 - Advanced document conversion
- **OCR Engines**: EasyOCR v1.7.0, Tesseract OCR
- **Supported Formats**: PDF, DOCX, XLSX, TXT, Images

#### Frameworks & Libraries
- **API Framework**: FastAPI 0.110.0
- **LLM Framework**: LangChain + LangGraph
- **Deep Learning**: PyTorch 2.2.2+cu121
- **Transformers**: Hugging Face Transformers 4.44.2
- **Language**: Python 3.11+

#### Infrastructure
- **Containerization**: Docker & Docker Compose
- **GPU Support**: CUDA 12.1.1 + cuDNN 8

## C·∫•u tr√∫c d·ª± √°n

```
.
‚îú‚îÄ‚îÄ Embedding_vectorDB/          # Document Processing Service
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # C·∫•u h√¨nh x·ª≠ l√Ω t√†i li·ªáu
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py   # Smart chunking + legacy processing
‚îÇ   ‚îú‚îÄ‚îÄ docling_processor.py    # Docling integration
‚îÇ   ‚îú‚îÄ‚îÄ embedding_service.py    # Vietnamese SBERT embeddings
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI endpoints
‚îÇ   ‚îî‚îÄ‚îÄ milvus_client.py        # Vector database client
‚îÇ
‚îú‚îÄ‚îÄ RAG_Core/                    # RAG Multi-Agent System
‚îÇ   ‚îú‚îÄ‚îÄ agents/                 # Agent implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ supervisor.py      # Context-aware routing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faq_agent.py       # FAQ with multi-variant reranking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever_agent.py # Document retrieval
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grader_agent.py    # BGE reranker integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generator_agent.py # Streaming response generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py      # Streaming-enabled base classes
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ models/                # Model wrappers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding_model.py # Vietnamese SBERT
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_model.py       # Ollama LLM with streaming
‚îÇ   ‚îú‚îÄ‚îÄ database/              # Database clients
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ milvus_client.py   # Milvus operations
‚îÇ   ‚îú‚îÄ‚îÄ tools/                 # LangChain tools
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector_search.py   # Search + Vietnamese reranking
‚îÇ   ‚îú‚îÄ‚îÄ workflow/              # LangGraph workflow
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_workflow.py    # Parallel execution + streaming
‚îÇ   ‚îú‚îÄ‚îÄ api/                   # FastAPI endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py            # Streaming SSE endpoint
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py         # Request/response models
‚îÇ   ‚îú‚îÄ‚îÄ config/                # Configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py        # Environment settings
‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context_processor.py # Context extraction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ helpers.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py                # Entry point
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml          # Multi-service orchestration
‚îú‚îÄ‚îÄ Dockerfile                  # CUDA + Python environment
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îî‚îÄ‚îÄ README.md                   # This file
```

## H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t v√† ch·∫°y

### Y√™u c·∫ßu h·ªá th·ªëng
- **OS**: Linux (Ubuntu 22.04 recommended)
- **Python**: 3.11+
- **Docker**: 24.0+ & Docker Compose v2
- **GPU**: NVIDIA GPU v·ªõi √≠t nh·∫•t 16GB VRAM (ƒë·ªÉ ch·∫°y GPT-OSS-20B)
- **CUDA**: 12.1+ with cuDNN 8
- **RAM**: T·ªëi thi·ªÉu 32GB
- **Storage**: T·ªëi thi·ªÉu 100GB free space (cho models v√† volumes)

### B∆∞·ªõc 1: Clone repository

```bash
git clone <repository-url>
cd <project-directory>
```

### B∆∞·ªõc 2: C·∫•u h√¨nh m√¥i tr∆∞·ªùng (Optional)

T·∫°o file `.env` ƒë·ªÉ override default settings:

```bash
# Milvus Configuration
MILVUS_HOST=milvus
MILVUS_PORT=19530

# Ollama Configuration
OLLAMA_URL=http://ollama:11434
LLM_MODEL=gpt-oss:20b

# Embedding Configuration
EMBEDDING_MODEL=keepitreal/vietnamese-sbert
EMBEDDING_DIM=768

# Search Configuration
SIMILARITY_THRESHOLD=0.2
TOP_K=20

# FAQ Optimization
FAQ_VECTOR_THRESHOLD=0.5
FAQ_RERANK_THRESHOLD=0.6
FAQ_TOP_K=10

# Document Grader
DOCUMENT_RERANK_THRESHOLD=0.7

# Support Contact
SUPPORT_PHONE="Ph√≤ng v·∫≠n h√†nh 0904540490 - Ph√≤ng kinh doanh:0914616081"
```

### B∆∞·ªõc 3: Kh·ªüi ƒë·ªông h·ªá th·ªëng v·ªõi Docker Compose

```bash
# Build v√† kh·ªüi ƒë·ªông t·∫•t c·∫£ services
docker-compose up -d

# Xem logs ƒë·ªÉ theo d√µi qu√° tr√¨nh kh·ªüi ƒë·ªông
docker-compose logs -f

# Ki·ªÉm tra tr·∫°ng th√°i containers
docker-compose ps
```

**Services s·∫Ω kh·ªüi ƒë·ªông theo th·ª© t·ª±:**
1. etcd (coordination)
2. MinIO (object storage)
3. Milvus (vector database)
4. Attu (Milvus UI)
5. Ollama (LLM server)
6. ollama-init (auto-pull gpt-oss:20b model)
7. document-api (port 8000)
8. rag-api (port 8501)

**Th·ªùi gian kh·ªüi ƒë·ªông d·ª± ki·∫øn**: 5-10 ph√∫t (ph·ª• thu·ªôc v√†o network speed khi pull model GPT-OSS-20B ~12GB)

### B∆∞·ªõc 4: X√°c minh c√°c services

```bash
# Check Milvus
curl http://localhost:9091/healthz

# Check Ollama
curl http://localhost:11434/api/tags

# Check Document API
curl http://localhost:8000/api/v1/health

# Check RAG API
curl http://localhost:8501/health
```

### B∆∞·ªõc 5: Truy c·∫≠p Web UIs

- **Attu (Milvus UI)**: http://localhost:3000
- **MinIO Console**: http://localhost:9001 (minioadmin/minioadmin)
- **Document API Docs**: http://localhost:8000/docs
- **RAG API Docs**: http://localhost:8501/docs

## üìù S·ª≠ d·ª•ng h·ªá th·ªëng

### 1. Process Documents (Document API)

#### Process PDF/DOCX/XLSX/TXT
```bash
curl -X POST "http://localhost:8000/api/v1/process-document" \
  -F "file=@document.pdf"
```

**Response:**
```json
{
  "status": "success",
  "filename": "document.pdf",
  "markdown_content": "# Title\n## Section\n...",
  "processing_info": {
    "file_type": ".pdf",
    "content_length": 1524,
    "processor": "docling+smart_chunker"
  }
}
```

#### Embed Markdown v·ªõi Smart Chunking
```bash
curl -X POST "http://localhost:8000/api/v1/embed-markdown" \
  -H "Content-Type: application/json" \
  -d '{
    "markdown_content": "# Title\nContent...",
    "document_id": "doc_001",
    "chunk_mode": "smart"
  }'
```

**Chunk Modes:**
- `smart` (recommended): Semantic chunking v·ªõi context preservation
- `sentence`: Legacy sentence-based chunking
- `legacy`: Backward compatible mode

### 2. Add FAQs
```bash
curl -X POST "http://localhost:8000/api/v1/faq/add" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "L√†m sao ƒë·ªÉ reset m·∫≠t kh·∫©u?",
    "answer": "B·∫°n click v√†o Qu√™n m·∫≠t kh·∫©u v√† l√†m theo h∆∞·ªõng d·∫´n",
    "faq_id": "faq_001"
  }'
```

### 3. Chat v·ªõi RAG System

#### Non-Streaming Mode
```bash
curl -X POST "http://localhost:8501/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "L√†m th·∫ø n√†o ƒë·ªÉ ƒëƒÉng k√Ω d·ªãch v·ª•?",
    "history": [],
    "stream": false
  }'
```

#### Streaming Mode (SSE)
```bash
curl -X POST "http://localhost:8501/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "L√†m th·∫ø n√†o ƒë·ªÉ ƒëƒÉng k√Ω d·ªãch v·ª•?",
    "history": [],
    "stream": true
  }'
```

**Streaming Response Format:**
```
data: {"type":"start","content":null,"references":null,"status":"processing"}

data: {"type":"chunk","content":"ƒê·ªÉ ƒëƒÉng k√Ω","references":null,"status":null}

data: {"type":"chunk","content":" d·ªãch v·ª•","references":null,"status":null}

data: {"type":"references","content":null,"references":[...],"status":null}

data: {"type":"end","content":null,"references":null,"status":"SUCCESS"}
```

### 4. Python Client Example

```python
from chat_client import StreamingChatClient

# Initialize client
client = StreamingChatClient("http://localhost:8501/")

# Check health
client.check_health()

# Send streaming message
client.send_message_streaming("Tr·ª£ l√Ω ·∫£o l√† g√¨?")

# Send non-streaming message
client.send_message_non_streaming("L√†m sao s·ª≠ d·ª•ng AI?")

# Interactive mode
client.interactive_mode()
```

## Configuration

### Agent Behavior Tuning

#### FAQ Agent (`config/settings.py`)
```python
# Vector search phase
FAQ_VECTOR_THRESHOLD: float = 0.5  # Lower = more candidates
FAQ_TOP_K: int = 10                # Number of candidates for reranking

# Reranking phase
FAQ_RERANK_THRESHOLD: float = 0.6  # Higher = more confident

# Reranking weights (must sum to 1.0)
FAQ_QUESTION_WEIGHT: float = 0.5   # Question-only matching
FAQ_QA_WEIGHT: float = 0.3         # Question+Answer matching
FAQ_ANSWER_WEIGHT: float = 0.2     # Answer-only matching

# Bonus for consistent scores
FAQ_CONSISTENCY_BONUS: float = 1.1
FAQ_CONSISTENCY_THRESHOLD: float = 0.75
```

#### Document Grader
```python
DOCUMENT_RERANK_THRESHOLD: float = 0.7
```

#### General Search
```python
SIMILARITY_THRESHOLD: float = 0.2
TOP_K: int = 20
```

### Custom Prompts

Ch·ªânh s·ª≠a prompts trong agent files:

- **Supervisor**: `agents/supervisor.py` - Classification prompt
- **FAQ**: `agents/faq_agent.py` - FAQ answering prompt
- **Generator**: `agents/generator_agent.py` - Answer generation prompts
- **Chatter**: `agents/chatter_agent.py` - Emotional support prompt
- **Not Enough Info**: `agents/not_enough_info_agent.py` - Fallback response prompt

### Docling Configuration

```python
# config.py in Embedding_vectorDB
USE_DOCLING: bool = True          # Enable/disable Docling
USE_OCR: bool = True              # Enable OCR for scanned docs
OCR_LANGUAGES: str = "vi,en"      # OCR languages
OCR_USE_GPU: bool = True          # Use GPU for OCR
```

### Smart Chunking Configuration

```python
# document_processor.py
SmartChunker(
    target_chunk_size=500,        # Target tokens per chunk
    min_chunk_size=200,           # Minimum chunk size
    max_chunk_size=800,           # Maximum chunk size
    overlap_size=100              # Overlap between chunks
)
```

## Development & Testing

### Run Document API Tests
```bash
cd Embedding_vectorDB
python test_API.py
```

### Run RAG Chat Client
```bash
cd RAG_Core
python chat_client.py

# Or test specific question
python chat_client.py "Tr·ª£ l√Ω ·∫£o l√† g√¨?"
```

### Monitor Logs
```bash
# All services
docker-compose logs -f

# Specific service
docker-compose logs -f rag-api
docker-compose logs -f document-api
docker-compose logs -f ollama
```

### Debug Mode

Enable debug logging in `api/main.py`:
```python
logging.basicConfig(
    level=logging.DEBUG,  # Change from INFO to DEBUG
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

## Troubleshooting

### Ollama Model Not Loading
```bash
# Check if model exists
docker exec ollama ollama list

# Manually pull model
docker exec ollama ollama pull gpt-oss:20b
```

### Milvus Connection Issues
```bash
# Check Milvus health
curl http://localhost:9091/healthz

# Restart Milvus
docker-compose restart milvus
```

### GPU Not Detected
```bash
# Verify NVIDIA runtime
docker run --rm --gpus all nvidia/cuda:12.1.1-base-ubuntu22.04 nvidia-smi

# Check Docker GPU support
docker info | grep -i gpu
```

### Document Processing Fails
```bash
# Check document-api logs
docker-compose logs document-api

# Verify Tesseract installation
docker exec document-api tesseract --version
```

### Slow Response Times
- Check GPU utilization: `nvidia-smi`
- Reduce `TOP_K` in settings
- Increase `SIMILARITY_THRESHOLD` to filter more aggressively
- Use FAQ mode instead of full RAG when possible

## Performance Optimization

### Parallel Execution
The system uses parallel execution for:
- Supervisor classification
- FAQ search
- Document retrieval

Adjust worker threads in `rag_workflow.py`:
```python
self.executor = ThreadPoolExecutor(
    max_workers=3,  # Increase for more parallelism
    thread_name_prefix="RAG-Worker"
)
```

### Context Caching
Context processor uses LRU cache:
```python
ContextProcessor(
    max_context_length=500,
    cache_size=10  # Increase for more caching
)
```

### Streaming Latency
Adjust streaming chunk delays in `api/main.py`:
```python
await asyncio.sleep(0.001)  # Reduce for faster streaming
```

## Security Considerations

- Change default MinIO credentials in production
- Use environment variables for sensitive configs
- Enable authentication for Milvus in production
- Implement rate limiting on API endpoints
- Use HTTPS in production deployment

## Production Deployment

### Resource Requirements
- **CPU**: 8+ cores
- **RAM**: 64GB+
- **GPU**: NVIDIA A100 or similar (24GB+ VRAM)
- **Storage**: 500GB+ SSD

### Docker Compose Production Override
Create `docker-compose.prod.yml`:
```yaml
version: '3.8'

services:
  rag-api:
    restart: always
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 16G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

Deploy:
```bash
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d
```

## API Documentation

Full API documentation available at:
- Document API: http://localhost:8000/docs
- RAG API: http://localhost:8501/docs

## Key Features Summary

**Smart Document Processing**
- Docling integration for high-quality conversion
- Semantic chunking with context preservation
- Multi-format support (PDF, DOCX, XLSX, TXT)

**Vietnamese-Optimized RAG**
- Vietnamese SBERT embeddings (768D)
- BGE Reranker v2-m3 for Vietnamese
- Multi-variant FAQ matching

**Streaming Support**
- Server-Sent Events (SSE) streaming
- True async streaming from LLM
- Progressive answer generation

**Multi-Agent Architecture**
- Parallel execution (Supervisor + FAQ + Retriever)
- Context-aware routing
- Specialized agents for different scenarios

**Production Ready**
- Docker Compose orchestration
- Health checks and auto-restart
- GPU acceleration
- Comprehensive logging

## Contributing

Contributions are welcome! Please follow these guidelines:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

[Your License Here]

---

** Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi RAG Multi-Agent Chatbot!**

For issues and questions, please create an issue on GitHub or contact the maintainers.
